{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5c331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cloud_storage.redshift_connection import redshift_connection\n",
    "\n",
    "connection = redshift_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e7a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 12:22:06,384 - side - DEBUG - Attempting to read SQL file: sql_files/item_query.sql\n",
      "2025-08-18 12:22:06,386 - side - INFO - Successfully read SQL file: sql_files/item_query.sql\n",
      "2025-08-18 12:22:09,190 - read_shift - INFO - Connected to Redshift successfully.\n",
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/cloud_storage/redshift_connection.py:82: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "2025-08-18 12:23:16,082 - read_shift - INFO - Query executed successfully, retrieved 12189 rows.\n",
      "2025-08-18 12:23:16,083 - read_shift - INFO - Connection closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>book_isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>authors</th>\n",
       "      <th>book_series</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>rights</th>\n",
       "      <th>illustrators</th>\n",
       "      <th>interactive</th>\n",
       "      <th>search_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>clicks_students</th>\n",
       "      <th>quality_clicks</th>\n",
       "      <th>quality_clicks_students</th>\n",
       "      <th>students_completed_book</th>\n",
       "      <th>students_completed_75_per_book</th>\n",
       "      <th>per_75_completed_unique_books</th>\n",
       "      <th>completion_rate</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>total_pages</th>\n",
       "      <th>read_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>9780778792352</td>\n",
       "      <td>Animal Rights Activist</td>\n",
       "      <td>Carrie,Gleason</td>\n",
       "      <td>Get Involved!</td>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>601.0</td>\n",
       "      <td>537.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>42.73</td>\n",
       "      <td>339532.0</td>\n",
       "      <td>24552.0</td>\n",
       "      <td>10704.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92</td>\n",
       "      <td>9781427166760</td>\n",
       "      <td>Cheerleading in Action</td>\n",
       "      <td>John,Crossingham</td>\n",
       "      <td>Sports in Action</td>\n",
       "      <td>2003-03-15</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>217.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>43.16</td>\n",
       "      <td>22385.0</td>\n",
       "      <td>10974.0</td>\n",
       "      <td>5800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>9781427164278</td>\n",
       "      <td>Classroom Games</td>\n",
       "      <td>Bobbie,Kalman</td>\n",
       "      <td>Historic Communities</td>\n",
       "      <td>2000-10-31</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>214.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>30.13</td>\n",
       "      <td>12652.0</td>\n",
       "      <td>8608.0</td>\n",
       "      <td>2838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>210</td>\n",
       "      <td>9781638973744</td>\n",
       "      <td>Forces and Changes in Motion</td>\n",
       "      <td>Christina,Earley</td>\n",
       "      <td>Physical Science</td>\n",
       "      <td>2022-02-01</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Forces, Friction, Magnet, Motion, Physical Sci...</td>\n",
       "      <td>...</td>\n",
       "      <td>506.0</td>\n",
       "      <td>543.0</td>\n",
       "      <td>453.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>78.43</td>\n",
       "      <td>277827.0</td>\n",
       "      <td>13545.0</td>\n",
       "      <td>9573.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>286</td>\n",
       "      <td>9781427191458</td>\n",
       "      <td>Introducing the Periodic Table</td>\n",
       "      <td>Tom,Jackson</td>\n",
       "      <td>Why Chemistry Matters</td>\n",
       "      <td>2012-10-30</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>59.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>39.22</td>\n",
       "      <td>14799.0</td>\n",
       "      <td>2542.0</td>\n",
       "      <td>1169.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      book_isbn                      book_title           authors  \\\n",
       "0   36  9780778792352          Animal Rights Activist    Carrie,Gleason   \n",
       "1   92  9781427166760          Cheerleading in Action  John,Crossingham   \n",
       "2  101  9781427164278                 Classroom Games     Bobbie,Kalman   \n",
       "3  210  9781638973744    Forces and Changes in Motion  Christina,Earley   \n",
       "4  286  9781427191458  Introducing the Periodic Table       Tom,Jackson   \n",
       "\n",
       "             book_series publication_date rights illustrators interactive  \\\n",
       "0          Get Involved!       2009-08-01  World         None       False   \n",
       "1       Sports in Action       2003-03-15  World         None       False   \n",
       "2   Historic Communities       2000-10-31  World         None       False   \n",
       "3       Physical Science       2022-02-01  World         None       False   \n",
       "4  Why Chemistry Matters       2012-10-30  World         None       False   \n",
       "\n",
       "                                     search_keywords  ... clicks_students  \\\n",
       "0                                               None  ...           601.0   \n",
       "1                                               None  ...           217.0   \n",
       "2                                               None  ...           214.0   \n",
       "3  Forces, Friction, Magnet, Motion, Physical Sci...  ...           506.0   \n",
       "4                                               None  ...            59.0   \n",
       "\n",
       "  quality_clicks quality_clicks_students students_completed_book  \\\n",
       "0          537.0                   437.0                   191.0   \n",
       "1          269.0                   187.0                    82.0   \n",
       "2          169.0                   148.0                    47.0   \n",
       "3          543.0                   453.0                   360.0   \n",
       "4           71.0                    51.0                    20.0   \n",
       "\n",
       "  students_completed_75_per_book per_75_completed_unique_books  \\\n",
       "0                          402.0                          89.0   \n",
       "1                          179.0                          94.0   \n",
       "2                          132.0                          84.0   \n",
       "3                          439.0                          95.0   \n",
       "4                           41.0                          80.0   \n",
       "\n",
       "  completion_rate time_spent  total_pages read_pages  \n",
       "0           42.73   339532.0      24552.0    10704.0  \n",
       "1           43.16    22385.0      10974.0     5800.0  \n",
       "2           30.13    12652.0       8608.0     2838.0  \n",
       "3           78.43   277827.0      13545.0     9573.0  \n",
       "4           39.22    14799.0       2542.0     1169.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_query = 'sql_files/item_query.sql'\n",
    "book_df = connection.redshift_query_fetching_as_df(item_query)\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d2d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tta/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.components.data_transformation import book_data_transformation,user_data_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11ba5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/components/data_transformation.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  book_df['long_description'].fillna('unk',inplace=True)\n",
      "Batches:   0%|          | 0/381 [00:00<?, ?it/s]/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|██████████| 381/381 [00:09<00:00, 38.33it/s]\n",
      "Batches:   0%|          | 0/381 [00:00<?, ?it/s]/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|██████████| 381/381 [00:38<00:00, 10.02it/s]\n",
      "Batches:   0%|          | 0/381 [00:00<?, ?it/s]/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|██████████| 381/381 [00:32<00:00, 11.55it/s]\n",
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/components/data_transformation.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved and merged into feature_mappings/theme_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/category_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/reading_skill_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/grades_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/book_code_to_idx.json\n"
     ]
    }
   ],
   "source": [
    "item_df,  book_feature_count, emb_count =  book_data_transformation(book_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbae67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from src.utils.main_utils import save_dict_to_json\n",
    "\n",
    "# log_dir = 'logs'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# logger = logging.getLogger('data_transformation')\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# console_handler = logging.StreamHandler()\n",
    "# console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# file_handler = logging.FileHandler(os.path.join(log_dir, 'data_transformation.log'), mode='a')\n",
    "# file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# console_handler.setFormatter(formatter)\n",
    "# file_handler.setFormatter(formatter)\n",
    "\n",
    "# logger.addHandler(console_handler)\n",
    "# logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def encode_column_with_sentence_transformer(df: pd.DataFrame, column: str, model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Encodes a column of text into embeddings using a sentence-transformer model.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input dataframe.\n",
    "#         column (str): Column name to encode.\n",
    "#         model_name (str): Pretrained sentence-transformers model name.\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Array of shape (num_rows, embedding_dim)\n",
    "#     \"\"\"\n",
    "#     model = SentenceTransformer(model_name)\n",
    "    \n",
    "#     # Fill missing values\n",
    "#     texts = df[column].fillna(\"unk\").astype(str).tolist()\n",
    "    \n",
    "#     # Encode with model\n",
    "#     embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "#     return np.array(embeddings)\n",
    "\n",
    "# def pre_train_emb_creation(book_df):\n",
    "\n",
    "#     book_df['title_plus_author'] = book_df.apply(lambda x:x['book_title'].lower()+' by '+x['authors'].lower(),axis=1)\n",
    "#     book_df['long_description'].fillna('unk',inplace=True)\n",
    "#     book_df['long_description'] = book_df.apply(lambda x:x['long_description'].lower(),axis=1)\n",
    "#     columns = ['book_isbn', 'title_plus_author', 'book_series', 'book_type', 'long_description','min_grade', 'max_grade',\n",
    "#         'readable_page_count','fiction_nonfiction', 'reading_skill_name','theme_name', 'category_name','language_book']\n",
    "\n",
    "#     book_df_final = book_df[columns]\n",
    "\n",
    "#     emb = encode_column_with_sentence_transformer(book_df_final,'title_plus_author')\n",
    "#     emb_df = pd.DataFrame(emb, columns=[f\"emb_title_author_{i}\" for i in range(emb.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     book_embedding_author_df = pd.concat([book_df_final, emb_df], axis=1)\n",
    "\n",
    "#     emb_desc = encode_column_with_sentence_transformer(book_embedding_author_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_desc_df = pd.DataFrame(emb_desc, columns=[f\"emb_desc_{i}\" for i in range(emb_desc.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     long_description_df = pd.concat([book_embedding_author_df, emb_desc_df], axis=1)\n",
    "#     emb_book_series = encode_column_with_sentence_transformer(long_description_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_book_series_df = pd.DataFrame(emb_book_series, columns=[f\"emb_book_series_{i}\" for i in range(emb_book_series.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "\n",
    "#     book_series_df = pd.concat([long_description_df,emb_book_series_df ], axis=1)\n",
    "\n",
    "#     return book_series_df , emb.shape[1] ,emb_desc.shape[1] ,emb_book_series.shape[1]\n",
    "\n",
    "# def clip(df,col,min,max):\n",
    "#     df[col] = np.clip(df[col],min,max)\n",
    "#     return df \n",
    "\n",
    "# def scaling(df, col ,value):\n",
    "#     df[col] = df[col]/value\n",
    "#     return df\n",
    "\n",
    "# grade_list = ['pk', 'k', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "# grade_to_idx = {g: i for i, g in enumerate(grade_list)}\n",
    "\n",
    "# def get_range(min_g, max_g):\n",
    "#     start_idx = grade_to_idx[min_g]\n",
    "#     end_idx = grade_to_idx[max_g]\n",
    "#     return ','.join(grade_list[start_idx:end_idx + 1])\n",
    "\n",
    "# def get_category_mapping(item_df, input_col, out_put_col,json_file_name):\n",
    "\n",
    "#     # Step 1: Preprocess themes (split on commas)\n",
    "#     item_df['themes'] = item_df[input_col].fillna('').apply(\n",
    "#         lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "#     )\n",
    "#     # Step 2: Build theme vocabulary\n",
    "#     from itertools import chain\n",
    "#     all_themes = sorted(set(chain.from_iterable(item_df['themes'])))\n",
    "#     theme_to_idx = {theme: idx for idx, theme in enumerate(all_themes)}\n",
    "#     if 'unk' not in theme_to_idx:\n",
    "#         theme_to_idx['unk'] = len(theme_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df['themes'].apply(\n",
    "#         lambda theme_list: [theme_to_idx[t] for t in theme_list if t in theme_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(theme_to_idx,json_file_name)\n",
    "#     return item_df, len(theme_to_idx)\n",
    "\n",
    "# def get_category_mapping_book(item_df,input_col ,out_put_col,json_file_name):\n",
    "\n",
    "#     book_code_to_idx = {theme: idx for idx, theme in enumerate(list((item_df[input_col])))}\n",
    "#     if 'unk' not in book_code_to_idx:\n",
    "#         book_code_to_idx['unk'] = len(book_code_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df[input_col].apply(\n",
    "#         lambda theme_list: [book_code_to_idx[t] for t in [theme_list] if t in book_code_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(book_code_to_idx,json_file_name)\n",
    "#     return item_df, len(book_code_to_idx)\n",
    "\n",
    "\n",
    "# # book_df_final['readable_page_count'] = np.clip(book_df_final['readable_page_count'],0,50)/50\n",
    "# # book_series_df.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def book_data_transformation(book_df):\n",
    "\n",
    "#     book_series_df, emb_shape,emb_desc_shape,emb_book_series_shape = pre_train_emb_creation(book_df)\n",
    "\n",
    "#     book_series_df = scaling(clip(book_series_df,'readable_page_count',0,50),'readable_page_count',50)\n",
    "\n",
    "#     book_series_df['book_type_binary'] = np.where(book_series_df.book_type == 'PDF',1,0)\n",
    "\n",
    "#     book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n",
    "#     book_df_final_v1 = pd.get_dummies(book_series_df, columns=['fiction_nonfiction'], prefix='fn')\n",
    "\n",
    "#     book_df_final_v1 = pd.get_dummies(book_df_final_v1, columns=['language_book'], prefix='lang')\n",
    "\n",
    "#     book_df_final_v1[\"grades\"] = book_df_final_v1.apply(\n",
    "#         lambda row: get_range(row[\"min_grade\"], row[\"max_grade\"]),axis=1)\n",
    "\n",
    "#     book_df_final_v1, theme_count = get_category_mapping(book_df_final_v1, 'theme_name', 'theme_ids','theme_to_idx.json')\n",
    "#     book_df_final_v1, category_count = get_category_mapping(book_df_final_v1, 'category_name', 'category_ids','category_to_idx.json')\n",
    "#     book_df_final_v1, reading_skills_count = get_category_mapping(book_df_final_v1, 'reading_skill_name', 'reading_skill_ids','reading_skill_to_idx.json')\n",
    "#     book_df_final_v1, grades_count = get_category_mapping(book_df_final_v1, 'grades', 'grades_ids','grades_to_idx.json')\n",
    "#     book_df_final_v1, book_count = get_category_mapping_book(book_df_final_v1,'book_isbn' ,'book_code_ids','book_code_to_idx.json')\n",
    "#     book_df_final_v1.rename(columns={'book_isbn':'book_code'},inplace=True)\n",
    "#     book_feature_count =  {\n",
    "#                     'themes_count':theme_count, \n",
    "#                     'book_count': book_count, \n",
    "#                     'grade_count': grades_count,\n",
    "#                     'reading_skills_count':reading_skills_count,\n",
    "#                     'category_count':category_count\n",
    "#                    }\n",
    "#     emb_count = {\n",
    "#                     'themes_count':8, \n",
    "#                     'book_count': 16, \n",
    "#                     'grade_count': 4,\n",
    "#                     'reading_skills_count':4,\n",
    "#                     'category_count':4\n",
    "#                 }\n",
    "    \n",
    "#     columns_author_title =[f\"emb_title_author_{i}\" for i in range(emb_shape)]\n",
    "#     columns_long_description = [f\"emb_desc_{i}\" for i in range(emb_desc_shape)]\n",
    "#     columns_book_series = [f\"emb_book_series_{i}\" for i in range(emb_book_series_shape)]\n",
    "#     columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "#         'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "#         'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "#     columns_learn_emb = ['book_code','book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "#     book_feature_cols = columns_learn_emb + columns_author_title + columns_long_description + columns_book_series + columns_add  \n",
    "\n",
    "#     return book_df_final_v1[book_feature_cols], book_feature_count, emb_count \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e7e28a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_code', 'book_code_ids', 'grades_ids', 'reading_skill_ids',\n",
       "       'category_ids', 'theme_ids', 'emb_title_author_0', 'emb_title_author_1',\n",
       "       'emb_title_author_2', 'emb_title_author_3',\n",
       "       ...\n",
       "       'book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
       "       'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
       "       'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish'],\n",
       "      dtype='object', length=1169)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7813b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from src.utils.main_utils import save_dict_to_json,  load_json_file\n",
    "\n",
    "\n",
    "# log_dir = 'logs'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# logger = logging.getLogger('data_transformation')\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# console_handler = logging.StreamHandler()\n",
    "# console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# file_handler = logging.FileHandler(os.path.join(log_dir, 'data_transformation.log'), mode='a')\n",
    "# file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# console_handler.setFormatter(formatter)\n",
    "# file_handler.setFormatter(formatter)\n",
    "\n",
    "# logger.addHandler(console_handler)\n",
    "# logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def encode_column_with_sentence_transformer(df: pd.DataFrame, column: str, model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Encodes a column of text into embeddings using a sentence-transformer model.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input dataframe.\n",
    "#         column (str): Column name to encode.\n",
    "#         model_name (str): Pretrained sentence-transformers model name.\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Array of shape (num_rows, embedding_dim)\n",
    "#     \"\"\"\n",
    "#     model = SentenceTransformer(model_name)\n",
    "    \n",
    "#     # Fill missing values\n",
    "#     texts = df[column].fillna(\"unk\").astype(str).tolist()\n",
    "    \n",
    "#     # Encode with model\n",
    "#     embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "#     return np.array(embeddings)\n",
    "\n",
    "# def pre_train_emb_creation(book_df):\n",
    "\n",
    "#     book_df['title_plus_author'] = book_df.apply(lambda x:x['book_title'].lower()+' by '+x['authors'].lower(),axis=1)\n",
    "#     book_df['long_description'].fillna('unk',inplace=True)\n",
    "#     book_df['long_description'] = book_df.apply(lambda x:x['long_description'].lower(),axis=1)\n",
    "#     columns = ['book_isbn', 'title_plus_author', 'book_series', 'book_type', 'long_description','min_grade', 'max_grade',\n",
    "#         'readable_page_count','fiction_nonfiction', 'reading_skill_name','theme_name', 'category_name','language_book']\n",
    "\n",
    "#     book_df_final = book_df[columns]\n",
    "\n",
    "#     emb = encode_column_with_sentence_transformer(book_df_final,'title_plus_author')\n",
    "#     emb_df = pd.DataFrame(emb, columns=[f\"emb_title_author_{i}\" for i in range(emb.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     book_embedding_author_df = pd.concat([book_df_final, emb_df], axis=1)\n",
    "\n",
    "#     emb_desc = encode_column_with_sentence_transformer(book_embedding_author_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_desc_df = pd.DataFrame(emb_desc, columns=[f\"emb_desc_{i}\" for i in range(emb_desc.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     long_description_df = pd.concat([book_embedding_author_df, emb_desc_df], axis=1)\n",
    "#     emb_book_series = encode_column_with_sentence_transformer(long_description_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_book_series_df = pd.DataFrame(emb_book_series, columns=[f\"emb_book_series_{i}\" for i in range(emb_book_series.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "\n",
    "#     book_series_df = pd.concat([long_description_df,emb_book_series_df ], axis=1)\n",
    "\n",
    "#     return book_series_df , emb.shape[1] ,emb_desc.shape[1] ,emb_book_series.shape[1]\n",
    "\n",
    "# def clip(df,col,min,max):\n",
    "#     df[col] = np.clip(df[col],min,max)\n",
    "#     return df \n",
    "\n",
    "# def scaling(df, col ,value):\n",
    "#     df[col] = df[col]/value\n",
    "#     return df\n",
    "\n",
    "# grade_list = ['pk', 'k', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "# grade_to_idx = {g: i for i, g in enumerate(grade_list)}\n",
    "\n",
    "# def get_range(min_g, max_g):\n",
    "#     start_idx = grade_to_idx[min_g]\n",
    "#     end_idx = grade_to_idx[max_g]\n",
    "#     return ','.join(grade_list[start_idx:end_idx + 1])\n",
    "\n",
    "# def get_category_mapping(item_df, input_col, out_put_col,json_file_name):\n",
    "\n",
    "#     # Step 1: Preprocess themes (split on commas)\n",
    "#     item_df['themes'] = item_df[input_col].fillna('').apply(\n",
    "#         lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "#     )\n",
    "#     # Step 2: Build theme vocabulary\n",
    "#     from itertools import chain\n",
    "#     all_themes = sorted(set(chain.from_iterable(item_df['themes'])))\n",
    "#     theme_to_idx = {theme: idx for idx, theme in enumerate(all_themes)}\n",
    "#     if 'unk' not in theme_to_idx:\n",
    "#         theme_to_idx['unk'] = len(theme_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df['themes'].apply(\n",
    "#         lambda theme_list: [theme_to_idx[t] for t in theme_list if t in theme_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(theme_to_idx,json_file_name)\n",
    "#     return item_df, len(theme_to_idx)\n",
    "\n",
    "# def get_category_mapping_book(item_df,input_col ,out_put_col,json_file_name):\n",
    "\n",
    "#     book_code_to_idx = {theme: idx for idx, theme in enumerate(list((item_df[input_col])))}\n",
    "#     if 'unk' not in book_code_to_idx:\n",
    "#         book_code_to_idx['unk'] = len(book_code_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df[input_col].apply(\n",
    "#         lambda theme_list: [book_code_to_idx[t] for t in [theme_list] if t in book_code_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(book_code_to_idx,json_file_name)\n",
    "#     return item_df, len(book_code_to_idx)\n",
    "\n",
    "\n",
    "# # book_df_final['readable_page_count'] = np.clip(book_df_final['readable_page_count'],0,50)/50\n",
    "# # book_series_df.shape\n",
    "\n",
    "# def last_10_books_fast(df):\n",
    "#     df = df.copy()\n",
    "#     df['book_create_dt'] = pd.to_datetime(df['book_create_dt'])\n",
    "#     df = df.sort_values(['user_id', 'book_create_dt']).reset_index(drop=True)\n",
    "\n",
    "#     # Helper to join last 10 values for each row in a group\n",
    "#     def last_10_join(series):\n",
    "#         out = []\n",
    "#         hist = []\n",
    "#         for val in series:\n",
    "#             out.append(','.join(hist[-10:]) if hist else 'unk')\n",
    "#             hist.append(val)\n",
    "#         return pd.Series(out, index=series.index)\n",
    "\n",
    "#     # Precompute category/theme strings\n",
    "#     # df['cat_str'] = df['category_name'].apply(lambda x: ','.join(x))\n",
    "#     # print(df['cat_str'])\n",
    "#     # df['theme_str'] = df['theme_name'].apply(lambda x: ','.join(x))\n",
    "#     df['cat_str'] = df['category_name']\n",
    "#     # print(df['cat_str'])\n",
    "#     df['theme_str'] = df['theme_name']\n",
    "#     df['rs_str'] = df['reading_skill_name']\n",
    "\n",
    "\n",
    "#     # Vectorized per-group computation (one Python loop per group, not per row globally)\n",
    "#     grouped = df.groupby('user_id', group_keys=False)\n",
    "#     df['last_10_books'] = grouped['book_code'].transform(last_10_join)\n",
    "#     df['last_category_name'] = grouped['cat_str'].transform(last_10_join)\n",
    "#     df['last_theme_name'] = grouped['theme_str'].transform(last_10_join)\n",
    "#     df['last_reading_skill_name'] = grouped['rs_str'].transform(last_10_join)\n",
    "\n",
    "#     return df.drop(columns=['cat_str', 'theme_str','rs_str'])\n",
    "\n",
    "# def get_mapping_user(child_df , input_col, output_col , file_path ):\n",
    "#     category_to_idx = load_json_file(file_path)\n",
    "#     child_df['last_categories'] = child_df[input_col].fillna('').apply(\n",
    "#     lambda x: [t.strip().lower() for t in x.split(',') if t.strip()])\n",
    "#     child_df[output_col] = child_df['last_categories'].apply(\n",
    "#     lambda theme_list: [category_to_idx[t] for t in theme_list if t in category_to_idx] )\n",
    "\n",
    "#     return child_df, len(category_to_idx)\n",
    "\n",
    "# def get_mapping_book_user(child_df , input_col, output_col , file_path ):\n",
    "#     book_code_to_idx = load_json_file(file_path)\n",
    "#     child_df['last_books_list'] = child_df[input_col].fillna('').apply(\n",
    "#     lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "# )\n",
    "\n",
    "#     child_df[output_col] = child_df['last_books_list'].apply(\n",
    "#     lambda theme_list: [book_code_to_idx[int(t)] if t!='unk' else book_code_to_idx[t]  for t in theme_list ])\n",
    "\n",
    "#     return child_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def book_data_transformation(book_df):\n",
    "\n",
    "#     book_series_df, emb_shape,emb_desc_shape,emb_book_series_shape = pre_train_emb_creation(book_df)\n",
    "\n",
    "#     book_series_df = scaling(clip(book_series_df,'readable_page_count',0,50),'readable_page_count',50)\n",
    "\n",
    "#     book_series_df['book_type_binary'] = np.where(book_series_df.book_type == 'PDF',1,0)\n",
    "\n",
    "#     book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n",
    "#     book_df_final_v1 = pd.get_dummies(book_series_df, columns=['fiction_nonfiction'], prefix='fn')\n",
    "\n",
    "#     book_df_final_v1 = pd.get_dummies(book_df_final_v1, columns=['language_book'], prefix='lang')\n",
    "\n",
    "#     book_df_final_v1[\"grades\"] = book_df_final_v1.apply(\n",
    "#         lambda row: get_range(row[\"min_grade\"], row[\"max_grade\"]),axis=1)\n",
    "\n",
    "#     book_df_final_v1, theme_count = get_category_mapping(book_df_final_v1, 'theme_name', 'theme_ids','theme_to_idx.json')\n",
    "#     book_df_final_v1, category_count = get_category_mapping(book_df_final_v1, 'category_name', 'category_ids','category_to_idx.json')\n",
    "#     book_df_final_v1, reading_skills_count = get_category_mapping(book_df_final_v1, 'reading_skill_name', 'reading_skill_ids','reading_skill_to_idx.json')\n",
    "#     book_df_final_v1, grades_count = get_category_mapping(book_df_final_v1, 'grades', 'grades_ids','grades_to_idx.json')\n",
    "#     book_df_final_v1, book_count = get_category_mapping_book(book_df_final_v1,'book_isbn' ,'book_code_ids','book_code_to_idx.json')\n",
    "#     book_df_final_v1.rename(columns={'book_isbn':'book_code'},inplace=True)\n",
    "#     book_feature_count =  {\n",
    "#                     'themes_count':theme_count, \n",
    "#                     'book_count': book_count, \n",
    "#                     'grade_count': grades_count,\n",
    "#                     'reading_skills_count':reading_skills_count,\n",
    "#                     'category_count':category_count\n",
    "#                    }\n",
    "#     emb_count = {\n",
    "#                     'themes_count':8, \n",
    "#                     'book_count': 16, \n",
    "#                     'grade_count': 4,\n",
    "#                     'reading_skills_count':4,\n",
    "#                     'category_count':4\n",
    "#                 }\n",
    "    \n",
    "#     columns_author_title =[f\"emb_title_author_{i}\" for i in range(emb_shape)]\n",
    "#     columns_long_description = [f\"emb_desc_{i}\" for i in range(emb_desc_shape)]\n",
    "#     columns_book_series = [f\"emb_book_series_{i}\" for i in range(emb_book_series_shape)]\n",
    "#     columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "#         'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "#         'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "#     columns_learn_emb = ['book_code','book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "#     book_feature_cols = columns_learn_emb + columns_author_title + columns_long_description + columns_book_series + columns_add  \n",
    "\n",
    "#     return book_df_final_v1[book_feature_cols], book_feature_count, emb_count \n",
    "\n",
    "\n",
    "# def user_data_transformation(user_df,user_loc,user_platform): \n",
    "#     # user_platform.rename(columns ={'isbn':'book_code'},inplace=True)\n",
    "#     df1 = user_platform.loc[:, ~user_platform.columns.duplicated()]\n",
    "#     user_platform = df1[['user_id', 'book_code', 'book_create_dt',\n",
    "#        'cumulative_web_during_school_hour',\n",
    "#        'cumulative_web_after_school_hour',\n",
    "#        'cumulative_apple_during_school_hour',\n",
    "#        'cumulative_apple_after_school_hour',\n",
    "#        'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "#        'cumulative_unk_during_school_hour',\n",
    "#        'cumulative_unk_after_school_hour']]\n",
    "    \n",
    "#     user_platform['total'] = user_platform['cumulative_web_during_school_hour'] + user_platform['cumulative_web_after_school_hour'] + user_platform['cumulative_apple_during_school_hour']+ user_platform['cumulative_apple_after_school_hour']+ user_platform['cumulative_android_during_school_hour']+ user_platform['cumulative_android_after_school_hour']+ user_platform['cumulative_unk_during_school_hour']+ user_platform['cumulative_unk_after_school_hour']\n",
    "\n",
    "#     user_platform['cumulative_web_during_school_hour'] = user_platform['cumulative_web_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_web_after_school_hour']  = user_platform['cumulative_web_after_school_hour'] /user_platform['total']\n",
    "#     user_platform['cumulative_apple_during_school_hour'] = user_platform['cumulative_apple_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_apple_after_school_hour'] = user_platform['cumulative_apple_after_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_android_during_school_hour'] = user_platform['cumulative_android_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_android_after_school_hour'] = user_platform['cumulative_android_after_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_unk_during_school_hour'] = user_platform['cumulative_unk_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_unk_after_school_hour']  = user_platform['cumulative_unk_after_school_hour']/user_platform['total']\n",
    "    \n",
    "#     user_platform_final = user_platform[['user_id', 'book_code',\n",
    "#        'cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "#        'cumulative_apple_during_school_hour',\n",
    "#        'cumulative_apple_after_school_hour',\n",
    "#        'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "#        'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "#        ]]\n",
    "\n",
    "#     user_df.dropna(subset=['category_name'],inplace=True)\n",
    "\n",
    "\n",
    "#     user_df['category_name'] = user_df['category_name'].fillna('unk')\n",
    "#     user_df['theme_name'] = user_df['theme_name'].fillna('unk')\n",
    "#     user_df['reading_skill_name'] = user_df['reading_skill_name'].fillna('unk')\n",
    "\n",
    "#     user_df['total_pages']=user_df['total_pages'].fillna(user_df['total_pages'].median())\n",
    "#     user_df['max_read_pages']=user_df['max_read_pages'].fillna(user_df['max_read_pages'].median())\n",
    "\n",
    "\n",
    "#     user_df_v1 = user_df[['book_code', 'user_id','category_name','theme_name','reading_skill_name', 'book_create_dt', 'total_pages',\n",
    "#        'max_read_pages']].copy()\n",
    "\n",
    "#     user_df_v1['book_create_dt'] = pd.to_datetime(user_df_v1['book_create_dt'])\n",
    "\n",
    "#     user_loc_v1 = user_loc[['user_id','country', 'state', 'zipcode','klass_grade_name','teacher_id','school_id','class_activation_bucket']].copy()\n",
    "    \n",
    "#     user_raw_df =  user_df_v1.merge(user_loc_v1, how ='left' ,on = 'user_id')\n",
    "\n",
    "#     user_platform_final['book_code'] = user_platform_final['book_code'].astype('str')\n",
    "\n",
    "#     user_raw_df_v1 = user_raw_df.merge(user_platform_final, how ='left' ,on = ['user_id','book_code'])\n",
    "\n",
    "#     cv = user_raw_df_v1 .copy()\n",
    "\n",
    "#     user_raw_df_v2 = last_10_books_fast(cv)\n",
    "#     user_raw_df_v2['class_activation_bucket'] = user_raw_df_v2['class_activation_bucket'].fillna('unk')\n",
    "#     user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['klass_grade_name'], prefix='grade')\n",
    "#     user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['class_activation_bucket'], prefix='class_activation_bucket')\n",
    "\n",
    "#     user_raw_df_v2['completion_rate'] = user_raw_df_v2['max_read_pages']/user_raw_df_v2['total_pages']\n",
    "#     user_raw_df_v2['label'] = np.where(user_raw_df_v2['completion_rate']>0.5,1,0)\n",
    "\n",
    "#     user_raw_df_v2, category_count  = get_mapping_user(user_raw_df_v2 , 'last_category_name', 'category_ids' , 'feature_mappings/category_to_idx.json')\n",
    "#     user_raw_df_v2, book_count = get_mapping_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "#     # user_raw_df_v2, book_count = get_mapping_book_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "#     user_raw_df_v2, reading_skills_count  = get_mapping_user(user_raw_df_v2 , 'last_reading_skill_name', 'reading_skill_ids' , 'feature_mappings/reading_skill_to_idx.json')\n",
    "#     user_raw_df_v2, theme_count = get_mapping_user(user_raw_df_v2 , 'last_theme_name', 'theme_ids' , 'feature_mappings/theme_to_idx.json')\n",
    "#     user_raw_df_v2, country_count = get_category_mapping(user_raw_df_v2, 'country', 'countries_ids','country_to_idx.json')\n",
    "#     user_raw_df_v2, state_count = get_category_mapping(user_raw_df_v2, 'state', 'states_ids','state_to_idx.json')\n",
    "#     user_raw_df_v2, zipcode_count = get_category_mapping(user_raw_df_v2, 'zipcode', 'zipcode_ids','zipcode_to_idx.json')\n",
    "#     user_raw_df_v2, teacher_count = get_category_mapping(user_raw_df_v2, 'teacher_id', 'teacher_code_ids','teacher_to_idx.json')\n",
    "#     user_raw_df_v2, school_count = get_category_mapping(user_raw_df_v2, 'school_id', 'school_code_ids','school_to_idx.json')\n",
    "\n",
    "#     user_columns = ['book_code', 'user_id', 'book_create_dt','book_code_ids','category_ids', 'state', 'zipcode',\n",
    "#        'teacher_id', 'school_id','cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "#        'cumulative_apple_during_school_hour',\n",
    "#        'cumulative_apple_after_school_hour',\n",
    "#        'cumulative_android_during_school_hour',\n",
    "#        'cumulative_android_after_school_hour',\n",
    "#        'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "#         'grade_grade 1', 'grade_grade 2', 'grade_grade 3',\n",
    "#        'grade_grade 4', 'grade_grade 5', 'grade_kindergarten', \n",
    "#        'class_activation_bucket_AC', 'class_activation_bucket_AC0',\n",
    "#        'class_activation_bucket_AC1', 'class_activation_bucket_AC2',\n",
    "#        'class_activation_bucket_AC3', 'class_activation_bucket_unk', 'last_10_books', 'last_category_name', 'last_theme_name',\n",
    "#        'last_reading_skill_name','label']\n",
    "\n",
    "#     user_feature_count =  {'themes_count':theme_count, \n",
    "#                    'book_count': book_count, \n",
    "#                    'reading_skills_count':reading_skills_count,\n",
    "#                    'category_count':category_count,\n",
    "#                    'country_count': country_count , \n",
    "#                     'state_count': state_count ,\n",
    "#                     'zipcode_count': zipcode_count,\n",
    "#                     'teacher_count': teacher_count,\n",
    "#                     'school_count': school_count\n",
    "#                    }\n",
    "#     user_emb_count = {\n",
    "#                 'themes_count':8, \n",
    "#                 'book_count': 16, \n",
    "#                 'reading_skills_count':4,\n",
    "#                 'category_count':4,\n",
    "#                 'country_count': 8 , \n",
    "#                 'state_count': 10,\n",
    "#                 'zipcode_count': 14,\n",
    "#                 'teacher_count': 16,\n",
    "#                 'school_count': 16\n",
    "#                 }\n",
    "\n",
    "#     return user_raw_df_v2[user_columns], user_feature_count, user_emb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a6da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 12:25:01,486 - side - DEBUG - Attempting to read SQL file: sql_files/user_book_platform.sql\n",
      "2025-08-18 12:25:01,487 - side - INFO - Successfully read SQL file: sql_files/user_book_platform.sql\n",
      "2025-08-18 12:25:04,398 - read_shift - INFO - Connected to Redshift successfully.\n",
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/cloud_storage/redshift_connection.py:82: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "2025-08-18 12:33:48,845 - read_shift - INFO - Query executed successfully, retrieved 4973201 rows.\n",
      "2025-08-18 12:33:48,851 - read_shift - INFO - Connection closed.\n",
      "2025-08-18 12:33:48,852 - side - DEBUG - Attempting to read SQL file: sql_files/user_location.sql\n",
      "2025-08-18 12:33:48,853 - side - INFO - Successfully read SQL file: sql_files/user_location.sql\n",
      "2025-08-18 12:33:51,399 - read_shift - INFO - Connected to Redshift successfully.\n",
      "2025-08-18 12:36:57,909 - read_shift - INFO - Query executed successfully, retrieved 658152 rows.\n",
      "2025-08-18 12:36:57,912 - read_shift - INFO - Connection closed.\n",
      "2025-08-18 12:36:57,913 - side - DEBUG - Attempting to read SQL file: sql_files/user_query.sql\n",
      "2025-08-18 12:36:57,915 - side - INFO - Successfully read SQL file: sql_files/user_query.sql\n",
      "2025-08-18 12:37:00,411 - read_shift - INFO - Connected to Redshift successfully.\n",
      "2025-08-18 12:41:56,901 - read_shift - INFO - Query executed successfully, retrieved 4973201 rows.\n",
      "2025-08-18 12:41:56,904 - read_shift - INFO - Connection closed.\n"
     ]
    }
   ],
   "source": [
    "platfrom_query = 'sql_files/user_book_platform.sql'\n",
    "user_platform = connection.redshift_query_fetching_as_df(platfrom_query)\n",
    "location_query = 'sql_files/user_location.sql'\n",
    "user_loc = connection.redshift_query_fetching_as_df(location_query)\n",
    "user_query = 'sql_files/user_query.sql'\n",
    "user_df = connection.redshift_query_fetching_as_df(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "996c6989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4973201 entries, 0 to 4973200\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   book_code           object        \n",
      " 1   user_id             object        \n",
      " 2   book_create_dt      datetime64[ns]\n",
      " 3   total_pages         float64       \n",
      " 4   max_read_pages      float64       \n",
      " 5   latest_to_old_rank  int64         \n",
      " 6   theme_name          object        \n",
      " 7   category_name       object        \n",
      " 8   reading_skill_name  object        \n",
      " 9   language_book       object        \n",
      " 10  book_series         object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(7)\n",
      "memory usage: 417.4+ MB\n"
     ]
    }
   ],
   "source": [
    "user_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34820abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4973201 entries, 0 to 4973200\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                 Dtype \n",
      "---  ------                                 ----- \n",
      " 0   user_id                                object\n",
      " 1   book_code                              object\n",
      " 2   book_create_dt                         object\n",
      " 3   cumulative_web_during_school_hour      int64 \n",
      " 4   cumulative_web_after_school_hour       int64 \n",
      " 5   cumulative_apple_during_school_hour    int64 \n",
      " 6   cumulative_apple_after_school_hour     int64 \n",
      " 7   cumulative_android_during_school_hour  int64 \n",
      " 8   cumulative_android_after_school_hour   int64 \n",
      " 9   cumulative_unk_during_school_hour      int64 \n",
      " 10  cumulative_unk_after_school_hour       int64 \n",
      "dtypes: int64(8), object(3)\n",
      "memory usage: 417.4+ MB\n"
     ]
    }
   ],
   "source": [
    "user_platform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5694368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 658152 entries, 0 to 658151\n",
      "Data columns (total 18 columns):\n",
      " #   Column                      Non-Null Count   Dtype         \n",
      "---  ------                      --------------   -----         \n",
      " 0   user_id                     658152 non-null  object        \n",
      " 1   teacher_creation_source     658152 non-null  object        \n",
      " 2   country                     657703 non-null  object        \n",
      " 3   state                       633667 non-null  object        \n",
      " 4   zipcode                     626528 non-null  object        \n",
      " 5   klass_grade_name            658152 non-null  object        \n",
      " 6   klass_id                    658152 non-null  object        \n",
      " 7   classroom_type              657789 non-null  object        \n",
      " 8   teacher_id                  658152 non-null  object        \n",
      " 9   teacher_create_dt           658152 non-null  datetime64[ns]\n",
      " 10  teacher_create_school_year  658152 non-null  float64       \n",
      " 11  school_id                   658083 non-null  object        \n",
      " 12  ac3                         530643 non-null  object        \n",
      " 13  ac2                         637530 non-null  datetime64[ns]\n",
      " 14  ac                          645793 non-null  datetime64[ns]\n",
      " 15  ac1                         654372 non-null  datetime64[ns]\n",
      " 16  ac0                         656321 non-null  datetime64[ns]\n",
      " 17  class_activation_bucket     656366 non-null  object        \n",
      "dtypes: datetime64[ns](5), float64(1), object(12)\n",
      "memory usage: 90.4+ MB\n"
     ]
    }
   ],
   "source": [
    "user_loc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2978270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa02f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_df.to_csv('user_interaction_data.csv',index=False)\n",
    "# user_loc.to_csv('user_location_data.csv',index=False)\n",
    "# user_platform.to_csv('user_platform_data.csv',index=False)\n",
    "user_df= pd.read_csv('user_interaction_data.csv')\n",
    "user_loc =pd.read_csv('user_location_data.csv')\n",
    "user_platform = pd.read_csv('user_platform_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013b4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4963473 entries, 0 to 4963472\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                 Dtype \n",
      "---  ------                                 ----- \n",
      " 0   user_id                                object\n",
      " 1   book_code                              int64 \n",
      " 2   book_create_dt                         object\n",
      " 3   cumulative_web_during_school_hour      int64 \n",
      " 4   cumulative_web_after_school_hour       int64 \n",
      " 5   cumulative_apple_during_school_hour    int64 \n",
      " 6   cumulative_apple_after_school_hour     int64 \n",
      " 7   cumulative_android_during_school_hour  int64 \n",
      " 8   cumulative_android_after_school_hour   int64 \n",
      " 9   cumulative_unk_during_school_hour      int64 \n",
      " 10  cumulative_unk_after_school_hour       int64 \n",
      "dtypes: int64(9), object(2)\n",
      "memory usage: 416.6+ MB\n"
     ]
    }
   ],
   "source": [
    "user_platform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e099ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils.main_utils import save_dict_to_json,  load_json_file\n",
    "\n",
    "\n",
    "log_dir = 'logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger('data_transformation')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "file_handler = logging.FileHandler(os.path.join(log_dir, 'data_transformation.log'), mode='a')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_column_with_sentence_transformer(df: pd.DataFrame, column: str, model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encodes a column of text into embeddings using a sentence-transformer model.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        column (str): Column name to encode.\n",
    "        model_name (str): Pretrained sentence-transformers model name.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (num_rows, embedding_dim)\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Fill missing values\n",
    "    texts = df[column].fillna(\"unk\").astype(str).tolist()\n",
    "    \n",
    "    # Encode with model\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def pre_train_emb_creation(book_df):\n",
    "\n",
    "    book_df['title_plus_author'] = book_df.apply(lambda x:x['book_title'].lower()+' by '+x['authors'].lower(),axis=1)\n",
    "    book_df['long_description'].fillna('unk',inplace=True)\n",
    "    book_df['long_description'] = book_df.apply(lambda x:x['long_description'].lower(),axis=1)\n",
    "    columns = ['book_isbn', 'title_plus_author', 'book_series', 'book_type', 'long_description','min_grade', 'max_grade',\n",
    "        'readable_page_count','fiction_nonfiction', 'reading_skill_name','theme_name', 'category_name','language_book']\n",
    "\n",
    "    book_df_final = book_df[columns]\n",
    "\n",
    "    emb = encode_column_with_sentence_transformer(book_df_final,'title_plus_author')\n",
    "    emb_df = pd.DataFrame(emb, columns=[f\"emb_title_author_{i}\" for i in range(emb.shape[1])])\n",
    "\n",
    "    # Combine with book_id\n",
    "    book_embedding_author_df = pd.concat([book_df_final, emb_df], axis=1)\n",
    "\n",
    "    emb_desc = encode_column_with_sentence_transformer(book_embedding_author_df,'long_description')\n",
    "    # Convert embeddings to DataFrame\n",
    "    emb_desc_df = pd.DataFrame(emb_desc, columns=[f\"emb_desc_{i}\" for i in range(emb_desc.shape[1])])\n",
    "\n",
    "    # Combine with book_id\n",
    "    long_description_df = pd.concat([book_embedding_author_df, emb_desc_df], axis=1)\n",
    "    emb_book_series = encode_column_with_sentence_transformer(long_description_df,'long_description')\n",
    "    # Convert embeddings to DataFrame\n",
    "    emb_book_series_df = pd.DataFrame(emb_book_series, columns=[f\"emb_book_series_{i}\" for i in range(emb_book_series.shape[1])])\n",
    "\n",
    "    # Combine with book_id\n",
    "\n",
    "    book_series_df = pd.concat([long_description_df,emb_book_series_df ], axis=1)\n",
    "\n",
    "    return book_series_df , emb.shape[1] ,emb_desc.shape[1] ,emb_book_series.shape[1]\n",
    "\n",
    "def clip(df,col,min,max):\n",
    "    df[col] = np.clip(df[col],min,max)\n",
    "    return df \n",
    "\n",
    "def scaling(df, col ,value):\n",
    "    df[col] = df[col]/value\n",
    "    return df\n",
    "\n",
    "grade_list = ['pk', 'k', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "grade_to_idx = {g: i for i, g in enumerate(grade_list)}\n",
    "\n",
    "def get_range(min_g, max_g):\n",
    "    start_idx = grade_to_idx[min_g]\n",
    "    end_idx = grade_to_idx[max_g]\n",
    "    return ','.join(grade_list[start_idx:end_idx + 1])\n",
    "\n",
    "def get_category_mapping(item_df, input_col, out_put_col,json_file_name):\n",
    "\n",
    "    # Step 1: Preprocess themes (split on commas)\n",
    "    item_df['themes'] = item_df[input_col].fillna('').apply(\n",
    "        lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "    )\n",
    "    # Step 2: Build theme vocabulary\n",
    "    from itertools import chain\n",
    "    all_themes = sorted(set(chain.from_iterable(item_df['themes'])))\n",
    "    theme_to_idx = {theme: idx for idx, theme in enumerate(all_themes)}\n",
    "    if 'unk' not in theme_to_idx:\n",
    "        theme_to_idx['unk'] = len(theme_to_idx)\n",
    "\n",
    "    # Step 3: Map themes to indices\n",
    "    item_df[out_put_col] = item_df['themes'].apply(\n",
    "        lambda theme_list: [theme_to_idx[t] for t in theme_list if t in theme_to_idx]\n",
    "    )\n",
    "    save_dict_to_json(theme_to_idx,json_file_name)\n",
    "    return item_df, len(theme_to_idx)\n",
    "\n",
    "def get_category_mapping_book(item_df,input_col ,out_put_col,json_file_name):\n",
    "\n",
    "    book_code_to_idx = {theme: idx for idx, theme in enumerate(list((item_df[input_col])))}\n",
    "    if 'unk' not in book_code_to_idx:\n",
    "        book_code_to_idx['unk'] = len(book_code_to_idx)\n",
    "\n",
    "    # Step 3: Map themes to indices\n",
    "    item_df[out_put_col] = item_df[input_col].apply(\n",
    "        lambda theme_list: [book_code_to_idx[t] for t in [theme_list] if t in book_code_to_idx]\n",
    "    )\n",
    "    save_dict_to_json(book_code_to_idx,json_file_name)\n",
    "    return item_df, len(book_code_to_idx)\n",
    "\n",
    "\n",
    "# book_df_final['readable_page_count'] = np.clip(book_df_final['readable_page_count'],0,50)/50\n",
    "# book_series_df.shape\n",
    "\n",
    "def last_10_books_fast(df):\n",
    "    df = df.copy()\n",
    "    df['book_create_dt'] = pd.to_datetime(df['book_create_dt'])\n",
    "    df = df.sort_values(['user_id', 'book_create_dt']).reset_index(drop=True)\n",
    "\n",
    "    # Helper to join last 10 values for each row in a group\n",
    "    def last_10_join(series):\n",
    "        out = []\n",
    "        hist = []\n",
    "        for val in series:\n",
    "            out.append(','.join(hist[-10:]) if hist else 'unk')\n",
    "            hist.append(val)\n",
    "        return pd.Series(out, index=series.index)\n",
    "\n",
    "    # Precompute category/theme strings\n",
    "    # df['cat_str'] = df['category_name'].apply(lambda x: ','.join(x))\n",
    "    # print(df['cat_str'])\n",
    "    # df['theme_str'] = df['theme_name'].apply(lambda x: ','.join(x))\n",
    "    df['cat_str'] = df['category_name']\n",
    "    # print(df['cat_str'])\n",
    "    df['theme_str'] = df['theme_name']\n",
    "    df['rs_str'] = df['reading_skill_name']\n",
    "\n",
    "\n",
    "    # Vectorized per-group computation (one Python loop per group, not per row globally)\n",
    "    grouped = df.groupby('user_id', group_keys=False)\n",
    "    df['last_10_books'] = grouped['book_code'].transform(last_10_join)\n",
    "    df['last_category_name'] = grouped['cat_str'].transform(last_10_join)\n",
    "    df['last_theme_name'] = grouped['theme_str'].transform(last_10_join)\n",
    "    df['last_reading_skill_name'] = grouped['rs_str'].transform(last_10_join)\n",
    "\n",
    "    return df.drop(columns=['cat_str', 'theme_str','rs_str'])\n",
    "\n",
    "def get_mapping_user(child_df , input_col, output_col , file_path ):\n",
    "    category_to_idx = load_json_file(file_path)\n",
    "    child_df['last_categories'] = child_df[input_col].fillna('').apply(\n",
    "    lambda x: [t.strip().lower() for t in x.split(',') if t.strip()])\n",
    "    child_df[output_col] = child_df['last_categories'].apply(\n",
    "    lambda theme_list: [category_to_idx[t] for t in theme_list if t in category_to_idx] )\n",
    "\n",
    "    return child_df, len(category_to_idx)\n",
    "\n",
    "def get_mapping_book_user(child_df , input_col, output_col , file_path ):\n",
    "    book_code_to_idx = load_json_file(file_path)\n",
    "    child_df['last_books_list'] = child_df[input_col].fillna('').apply(\n",
    "    lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    ")\n",
    "\n",
    "    child_df[output_col] = child_df['last_books_list'].apply(\n",
    "    lambda theme_list: [book_code_to_idx[int(t)] if t!='unk' else book_code_to_idx[t]  for t in theme_list ])\n",
    "\n",
    "    return child_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def book_data_transformation(book_df):\n",
    "\n",
    "    book_series_df, emb_shape,emb_desc_shape,emb_book_series_shape = pre_train_emb_creation(book_df)\n",
    "\n",
    "    book_series_df = scaling(clip(book_series_df,'readable_page_count',0,50),'readable_page_count',50)\n",
    "\n",
    "    book_series_df['book_type_binary'] = np.where(book_series_df.book_type == 'PDF',1,0)\n",
    "\n",
    "    book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n",
    "    book_df_final_v1 = pd.get_dummies(book_series_df, columns=['fiction_nonfiction'], prefix='fn')\n",
    "\n",
    "    book_df_final_v1 = pd.get_dummies(book_df_final_v1, columns=['language_book'], prefix='lang')\n",
    "\n",
    "    book_df_final_v1[\"grades\"] = book_df_final_v1.apply(\n",
    "        lambda row: get_range(row[\"min_grade\"], row[\"max_grade\"]),axis=1)\n",
    "\n",
    "    book_df_final_v1, theme_count = get_category_mapping(book_df_final_v1, 'theme_name', 'theme_ids','theme_to_idx.json')\n",
    "    book_df_final_v1, category_count = get_category_mapping(book_df_final_v1, 'category_name', 'category_ids','category_to_idx.json')\n",
    "    book_df_final_v1, reading_skills_count = get_category_mapping(book_df_final_v1, 'reading_skill_name', 'reading_skill_ids','reading_skill_to_idx.json')\n",
    "    book_df_final_v1, grades_count = get_category_mapping(book_df_final_v1, 'grades', 'grades_ids','grades_to_idx.json')\n",
    "    book_df_final_v1, book_count = get_category_mapping_book(book_df_final_v1,'book_isbn' ,'book_code_ids','book_code_to_idx.json')\n",
    "    book_df_final_v1.rename(columns={'book_isbn':'book_code'},inplace=True)\n",
    "    book_feature_count =  {\n",
    "                    'themes_count':theme_count, \n",
    "                    'book_count': book_count, \n",
    "                    'grade_count': grades_count,\n",
    "                    'reading_skills_count':reading_skills_count,\n",
    "                    'category_count':category_count\n",
    "                   }\n",
    "    emb_count = {\n",
    "                    'themes_count':8, \n",
    "                    'book_count': 16, \n",
    "                    'grade_count': 4,\n",
    "                    'reading_skills_count':4,\n",
    "                    'category_count':4\n",
    "                }\n",
    "    \n",
    "    columns_author_title =[f\"emb_title_author_{i}\" for i in range(emb_shape)]\n",
    "    columns_long_description = [f\"emb_desc_{i}\" for i in range(emb_desc_shape)]\n",
    "    columns_book_series = [f\"emb_book_series_{i}\" for i in range(emb_book_series_shape)]\n",
    "    columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "        'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "        'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "    columns_learn_emb = ['book_code','book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "    book_feature_cols = columns_learn_emb + columns_author_title + columns_long_description + columns_book_series + columns_add  \n",
    "\n",
    "    return book_df_final_v1[book_feature_cols], book_feature_count, emb_count \n",
    "\n",
    "\n",
    "def user_data_transformation(user_df,user_loc,user_platform): \n",
    "    # user_platform.rename(columns ={'isbn':'book_code'},inplace=True)\n",
    "    df1 = user_platform.copy()\n",
    "    user_platform = df1[['user_id', 'book_code', 'book_create_dt',\n",
    "       'cumulative_web_during_school_hour',\n",
    "       'cumulative_web_after_school_hour',\n",
    "       'cumulative_apple_during_school_hour',\n",
    "       'cumulative_apple_after_school_hour',\n",
    "       'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "       'cumulative_unk_during_school_hour',\n",
    "       'cumulative_unk_after_school_hour']]\n",
    "    \n",
    "    user_platform['total'] = user_platform['cumulative_web_during_school_hour'] + user_platform['cumulative_web_after_school_hour'] + user_platform['cumulative_apple_during_school_hour']+ user_platform['cumulative_apple_after_school_hour']+ user_platform['cumulative_android_during_school_hour']+ user_platform['cumulative_android_after_school_hour']+ user_platform['cumulative_unk_during_school_hour']+ user_platform['cumulative_unk_after_school_hour']\n",
    "\n",
    "    user_platform['cumulative_web_during_school_hour'] = user_platform['cumulative_web_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_web_after_school_hour']  = user_platform['cumulative_web_after_school_hour'] /user_platform['total']\n",
    "    user_platform['cumulative_apple_during_school_hour'] = user_platform['cumulative_apple_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_apple_after_school_hour'] = user_platform['cumulative_apple_after_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_android_during_school_hour'] = user_platform['cumulative_android_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_android_after_school_hour'] = user_platform['cumulative_android_after_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_unk_during_school_hour'] = user_platform['cumulative_unk_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_unk_after_school_hour']  = user_platform['cumulative_unk_after_school_hour']/user_platform['total']\n",
    "    \n",
    "    user_platform_final = user_platform[['user_id', 'book_code',\n",
    "       'cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "       'cumulative_apple_during_school_hour',\n",
    "       'cumulative_apple_after_school_hour',\n",
    "       'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "       'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "       ]]\n",
    "\n",
    "    user_df.dropna(subset=['category_name'],inplace=True)\n",
    "\n",
    "\n",
    "    user_df['category_name'] = user_df['category_name'].fillna('unk')\n",
    "    user_df['theme_name'] = user_df['theme_name'].fillna('unk')\n",
    "    user_df['reading_skill_name'] = user_df['reading_skill_name'].fillna('unk')\n",
    "\n",
    "    user_df['total_pages']=user_df['total_pages'].fillna(user_df['total_pages'].median())\n",
    "    user_df['max_read_pages']=user_df['max_read_pages'].fillna(user_df['max_read_pages'].median())\n",
    "\n",
    "\n",
    "    user_df_v1 = user_df[['book_code', 'user_id','category_name','theme_name','reading_skill_name', 'book_create_dt', 'total_pages',\n",
    "       'max_read_pages']].copy()\n",
    "\n",
    "    user_df_v1['book_create_dt'] = pd.to_datetime(user_df_v1['book_create_dt'])\n",
    "\n",
    "    user_loc_v1 = user_loc[['user_id','country', 'state', 'zipcode','klass_grade_name','teacher_id','school_id','class_activation_bucket']].copy()\n",
    "    \n",
    "    user_raw_df =  user_df_v1.merge(user_loc_v1, how ='left' ,on = 'user_id')\n",
    "\n",
    "   #  user_platform_final['book_code'] = user_platform_final['book_code'].astype('str')\n",
    "\n",
    "    user_raw_df_v1 = user_raw_df.merge(user_platform_final, how ='left' ,on = ['user_id','book_code'])\n",
    "\n",
    "    cv = user_raw_df_v1 .copy()\n",
    "\n",
    "    user_raw_df_v2 = last_10_books_fast(cv)\n",
    "    user_raw_df_v2['class_activation_bucket'] = user_raw_df_v2['class_activation_bucket'].fillna('unk')\n",
    "    user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['klass_grade_name'], prefix='grade')\n",
    "    user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['class_activation_bucket'], prefix='class_activation_bucket')\n",
    "\n",
    "    user_raw_df_v2['completion_rate'] = user_raw_df_v2['max_read_pages']/user_raw_df_v2['total_pages']\n",
    "    user_raw_df_v2['label'] = np.where(user_raw_df_v2['completion_rate']>0.5,1,0)\n",
    "\n",
    "    user_raw_df_v2, category_count  = get_mapping_user(user_raw_df_v2 , 'last_category_name', 'category_ids' , 'feature_mappings/category_to_idx.json')\n",
    "    user_raw_df_v2, book_count = get_mapping_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "    # user_raw_df_v2, book_count = get_mapping_book_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "    user_raw_df_v2, reading_skills_count  = get_mapping_user(user_raw_df_v2 , 'last_reading_skill_name', 'reading_skill_ids' , 'feature_mappings/reading_skill_to_idx.json')\n",
    "    user_raw_df_v2, theme_count = get_mapping_user(user_raw_df_v2 , 'last_theme_name', 'theme_ids' , 'feature_mappings/theme_to_idx.json')\n",
    "    user_raw_df_v2, country_count = get_category_mapping(user_raw_df_v2, 'country', 'countries_ids','country_to_idx.json')\n",
    "    user_raw_df_v2, state_count = get_category_mapping(user_raw_df_v2, 'state', 'states_ids','state_to_idx.json')\n",
    "    user_raw_df_v2, zipcode_count = get_category_mapping(user_raw_df_v2, 'zipcode', 'zipcode_ids','zipcode_to_idx.json')\n",
    "    user_raw_df_v2, teacher_count = get_category_mapping(user_raw_df_v2, 'teacher_id', 'teacher_code_ids','teacher_to_idx.json')\n",
    "    user_raw_df_v2, school_count = get_category_mapping(user_raw_df_v2, 'school_id', 'school_code_ids','school_to_idx.json')\n",
    "\n",
    "    user_columns = ['book_code', 'user_id', 'book_create_dt','book_code_ids','category_ids','reading_skill_ids','theme_ids','countries_ids', 'states_ids','zipcode_ids','teacher_code_ids','school_code_ids','state', 'zipcode',\n",
    "       'teacher_id', 'school_id','cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "       'cumulative_apple_during_school_hour',\n",
    "       'cumulative_apple_after_school_hour',\n",
    "       'cumulative_android_during_school_hour',\n",
    "       'cumulative_android_after_school_hour',\n",
    "       'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "        'grade_grade 1', 'grade_grade 2', 'grade_grade 3',\n",
    "       'grade_grade 4', 'grade_grade 5', 'grade_kindergarten', \n",
    "       'class_activation_bucket_AC', 'class_activation_bucket_AC0',\n",
    "       'class_activation_bucket_AC1', 'class_activation_bucket_AC2',\n",
    "       'class_activation_bucket_AC3', 'class_activation_bucket_unk', 'last_10_books', 'last_category_name', 'last_theme_name',\n",
    "       'last_reading_skill_name','label']\n",
    "\n",
    "    user_feature_count =  {'themes_count':theme_count, \n",
    "                   'book_count': book_count, \n",
    "                   'reading_skills_count':reading_skills_count,\n",
    "                   'category_count':category_count,\n",
    "                   'country_count': country_count , \n",
    "                    'state_count': state_count ,\n",
    "                    'zipcode_count': zipcode_count,\n",
    "                    'teacher_count': teacher_count,\n",
    "                    'school_count': school_count\n",
    "                   }\n",
    "    user_emb_count = {\n",
    "                'themes_count':8, \n",
    "                'book_count': 16, \n",
    "                'reading_skills_count':4,\n",
    "                'category_count':4,\n",
    "                'country_count': 8 , \n",
    "                'state_count': 10,\n",
    "                'zipcode_count': 14,\n",
    "                'teacher_count': 16,\n",
    "                'school_count': 16\n",
    "                }\n",
    "\n",
    "    return user_raw_df_v2[user_columns], user_feature_count, user_emb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c99ba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/components/data_transformation.py:298: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  user_platform_final['book_code'] = user_platform_final['book_code'].astype('str')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved and merged into feature_mappings/country_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/state_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/zipcode_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/teacher_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/school_to_idx.json\n"
     ]
    }
   ],
   "source": [
    "child_df,user_feature_count, user_emb_count= user_data_transformation(user_df,user_loc,user_platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8e6c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "book_feature_count, emb_count\n",
    "all_dicts = {\n",
    "    \"book_feature_count\": book_feature_count,\n",
    "    \"emb_count\": emb_count,\n",
    "    \"user_feature_count\": user_feature_count,\n",
    "    \"user_emb_count\": user_emb_count,\n",
    "    \"book_feature_dim\" : len(book_feature_cols),\n",
    "    \"user_feature_dim\" : len(interaction_feature_cols),\n",
    "}\n",
    "\n",
    "\n",
    "with open(\"model_parmater.json\", \"w\") as f:\n",
    "    json.dump(all_dicts, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299a60ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_code</th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_create_dt</th>\n",
       "      <th>book_code_ids</th>\n",
       "      <th>category_ids</th>\n",
       "      <th>reading_skill_ids</th>\n",
       "      <th>theme_ids</th>\n",
       "      <th>countries_ids</th>\n",
       "      <th>states_ids</th>\n",
       "      <th>zipcode_ids</th>\n",
       "      <th>...</th>\n",
       "      <th>class_activation_bucket_AC0</th>\n",
       "      <th>class_activation_bucket_AC1</th>\n",
       "      <th>class_activation_bucket_AC2</th>\n",
       "      <th>class_activation_bucket_AC3</th>\n",
       "      <th>class_activation_bucket_unk</th>\n",
       "      <th>last_10_books</th>\n",
       "      <th>last_category_name</th>\n",
       "      <th>last_theme_name</th>\n",
       "      <th>last_reading_skill_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781634401647</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-23 20:25:42.214126</td>\n",
       "      <td>[12189]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[107]</td>\n",
       "      <td>[147]</td>\n",
       "      <td>[59]</td>\n",
       "      <td>[13764]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9781039673106</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:01:06.548406</td>\n",
       "      <td>[3111]</td>\n",
       "      <td>[7]</td>\n",
       "      <td>[12]</td>\n",
       "      <td>[98, 41]</td>\n",
       "      <td>[147]</td>\n",
       "      <td>[59]</td>\n",
       "      <td>[13764]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647</td>\n",
       "      <td>Science &amp; Nature</td>\n",
       "      <td>The Natural World, Fun Science</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781039837843</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:02:48.414486</td>\n",
       "      <td>[3111, 3593]</td>\n",
       "      <td>[7, 1]</td>\n",
       "      <td>[12, 7, 5, 1]</td>\n",
       "      <td>[98, 41, 2, 57]</td>\n",
       "      <td>[147]</td>\n",
       "      <td>[59]</td>\n",
       "      <td>[13764]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647,9781039673106</td>\n",
       "      <td>Science &amp; Nature,Early Learning</td>\n",
       "      <td>The Natural World, Fun Science,Alphabet, Langu...</td>\n",
       "      <td>unk,Illustrations or other Visual Elements, De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781634409636</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:04:49.676354</td>\n",
       "      <td>[3111, 3593, 9293]</td>\n",
       "      <td>[7, 1, 5]</td>\n",
       "      <td>[12, 7, 5, 1, 7, 5, 1]</td>\n",
       "      <td>[98, 41, 2, 57, 73, 90]</td>\n",
       "      <td>[147]</td>\n",
       "      <td>[59]</td>\n",
       "      <td>[13764]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647,9781039673106,9781039837843</td>\n",
       "      <td>Science &amp; Nature,Early Learning,Growing Up</td>\n",
       "      <td>The Natural World, Fun Science,Alphabet, Langu...</td>\n",
       "      <td>unk,Illustrations or other Visual Elements, De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9781634409759</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:05:17.458130</td>\n",
       "      <td>[3111, 3593, 9293, 2811]</td>\n",
       "      <td>[7, 1, 5, 6]</td>\n",
       "      <td>[12, 7, 5, 1, 7, 5, 1, 11, 7, 6, 5, 3, 1]</td>\n",
       "      <td>[98, 41, 2, 57, 73, 90, 9]</td>\n",
       "      <td>[147]</td>\n",
       "      <td>[59]</td>\n",
       "      <td>[13764]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647,9781039673106,9781039837843,9781...</td>\n",
       "      <td>Science &amp; Nature,Early Learning,Growing Up,Peo...</td>\n",
       "      <td>The Natural World, Fun Science,Alphabet, Langu...</td>\n",
       "      <td>unk,Illustrations or other Visual Elements, De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       book_code                               user_id  \\\n",
       "0  9781634401647  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "1  9781039673106  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "2  9781039837843  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "3  9781634409636  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "4  9781634409759  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "\n",
       "              book_create_dt             book_code_ids  category_ids  \\\n",
       "0 2024-09-23 20:25:42.214126                   [12189]           [8]   \n",
       "1 2024-09-24 20:01:06.548406                    [3111]           [7]   \n",
       "2 2024-09-24 20:02:48.414486              [3111, 3593]        [7, 1]   \n",
       "3 2024-09-24 20:04:49.676354        [3111, 3593, 9293]     [7, 1, 5]   \n",
       "4 2024-09-24 20:05:17.458130  [3111, 3593, 9293, 2811]  [7, 1, 5, 6]   \n",
       "\n",
       "                           reading_skill_ids                   theme_ids  \\\n",
       "0                                       [12]                       [107]   \n",
       "1                                       [12]                    [98, 41]   \n",
       "2                              [12, 7, 5, 1]             [98, 41, 2, 57]   \n",
       "3                     [12, 7, 5, 1, 7, 5, 1]     [98, 41, 2, 57, 73, 90]   \n",
       "4  [12, 7, 5, 1, 7, 5, 1, 11, 7, 6, 5, 3, 1]  [98, 41, 2, 57, 73, 90, 9]   \n",
       "\n",
       "  countries_ids states_ids zipcode_ids  ... class_activation_bucket_AC0  \\\n",
       "0         [147]       [59]     [13764]  ...                       False   \n",
       "1         [147]       [59]     [13764]  ...                       False   \n",
       "2         [147]       [59]     [13764]  ...                       False   \n",
       "3         [147]       [59]     [13764]  ...                       False   \n",
       "4         [147]       [59]     [13764]  ...                       False   \n",
       "\n",
       "  class_activation_bucket_AC1 class_activation_bucket_AC2  \\\n",
       "0                       False                       False   \n",
       "1                       False                       False   \n",
       "2                       False                       False   \n",
       "3                       False                       False   \n",
       "4                       False                       False   \n",
       "\n",
       "  class_activation_bucket_AC3 class_activation_bucket_unk  \\\n",
       "0                        True                       False   \n",
       "1                        True                       False   \n",
       "2                        True                       False   \n",
       "3                        True                       False   \n",
       "4                        True                       False   \n",
       "\n",
       "                                       last_10_books  \\\n",
       "0                                                unk   \n",
       "1                                      9781634401647   \n",
       "2                        9781634401647,9781039673106   \n",
       "3          9781634401647,9781039673106,9781039837843   \n",
       "4  9781634401647,9781039673106,9781039837843,9781...   \n",
       "\n",
       "                                  last_category_name  \\\n",
       "0                                                unk   \n",
       "1                                   Science & Nature   \n",
       "2                    Science & Nature,Early Learning   \n",
       "3         Science & Nature,Early Learning,Growing Up   \n",
       "4  Science & Nature,Early Learning,Growing Up,Peo...   \n",
       "\n",
       "                                     last_theme_name  \\\n",
       "0                                                unk   \n",
       "1                     The Natural World, Fun Science   \n",
       "2  The Natural World, Fun Science,Alphabet, Langu...   \n",
       "3  The Natural World, Fun Science,Alphabet, Langu...   \n",
       "4  The Natural World, Fun Science,Alphabet, Langu...   \n",
       "\n",
       "                             last_reading_skill_name  label  \n",
       "0                                                unk      0  \n",
       "1                                                unk      1  \n",
       "2  unk,Illustrations or other Visual Elements, De...      1  \n",
       "3  unk,Illustrations or other Visual Elements, De...      0  \n",
       "4  unk,Illustrations or other Visual Elements, De...      1  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4709beea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_author_title =[f\"emb_title_author_{i}\" for i in range(384)]\n",
    "columns_long_description = [f\"emb_desc_{i}\" for i in range(384)]\n",
    "columns_book_series = [f\"emb_book_series_{i}\" for i in range(384)]\n",
    "columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "       'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "       'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "columns_learn_emb = ['book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "book_feature_cols = columns_author_title + columns_long_description + columns_book_series + columns_add\n",
    "\n",
    "interaction_feature_cols = ['cumulative_web_during_school_hour',\n",
    " 'cumulative_web_after_school_hour',\n",
    " 'cumulative_apple_during_school_hour',\n",
    " 'cumulative_apple_after_school_hour',\n",
    " 'cumulative_android_during_school_hour',\n",
    " 'cumulative_android_after_school_hour',\n",
    " 'cumulative_unk_during_school_hour',\n",
    " 'cumulative_unk_after_school_hour',\n",
    " 'grade_grade 1',\n",
    " 'grade_grade 2',\n",
    " 'grade_grade 3',\n",
    " 'grade_grade 4',\n",
    " 'grade_grade 5',\n",
    " 'grade_kindergarten',\n",
    " 'class_activation_bucket_AC',\n",
    " 'class_activation_bucket_AC0',\n",
    " 'class_activation_bucket_AC1',\n",
    " 'class_activation_bucket_AC2',\n",
    " 'class_activation_bucket_AC3',\n",
    " 'class_activation_bucket_unk',]\n",
    "\n",
    "\n",
    "\n",
    "len(book_feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769e25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.main_utils import  fast_split_user_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64a8ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test= fast_split_user_interactions(child_df[:100], user_col=\"user_id\", item_col=\"book_code\", time_col=\"book_create_dt\", min_interactions=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed669186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index= False)\n",
    "val.to_csv('val.csv',index= False)\n",
    "test.to_csv('test.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c724541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b92484",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  pd.read_csv('train.csv')\n",
    "val =  pd.read_csv('val.csv')\n",
    "test =  pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8838639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_code', 'user_id', 'book_create_dt', 'book_code_ids',\n",
       "       'category_ids', 'state', 'zipcode', 'teacher_id', 'school_id',\n",
       "       'cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
       "       'cumulative_apple_during_school_hour',\n",
       "       'cumulative_apple_after_school_hour',\n",
       "       'cumulative_android_during_school_hour',\n",
       "       'cumulative_android_after_school_hour',\n",
       "       'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
       "       'grade_grade 1', 'grade_grade 2', 'grade_grade 3', 'grade_grade 4',\n",
       "       'grade_grade 5', 'grade_kindergarten', 'class_activation_bucket_AC',\n",
       "       'class_activation_bucket_AC0', 'class_activation_bucket_AC1',\n",
       "       'class_activation_bucket_AC2', 'class_activation_bucket_AC3',\n",
       "       'class_activation_bucket_unk', 'last_10_books', 'last_category_name',\n",
       "       'last_theme_name', 'last_reading_skill_name', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25c829c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.data_loader import BookInteractionDataset, book_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610de7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3802477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = BookInteractionDataset(train[:5], item_df, book_feature_cols, interaction_feature_cols)\n",
    "trainloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=book_collate_fn)\n",
    "\n",
    "dataset = BookInteractionDataset( val[:5], item_df, book_feature_cols, interaction_feature_cols)\n",
    "valloader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=book_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52b9b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trainloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aca33a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.model import TwoTowerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc349f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'themes_count': 8,\n",
       " 'book_count': 16,\n",
       " 'grade_count': 4,\n",
       " 'reading_skills_count': 4,\n",
       " 'category_count': 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44c5f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTowerModel(book_feature_count, user_feature_count,\n",
    "                 emb_count, user_emb_count,\n",
    "                  book_feature_dim=len(book_feature_cols), user_feature_dim=len(interaction_feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f97359cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.model_trainer import train_two_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bca916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00, 88.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6769 | Val Loss: 0.6944\n",
      "Recall@3: 0.0000 | NDCG@3: 0.0000\n",
      "Recall@5: 1.0000 | NDCG@5: 0.4307\n",
      "✅ Validation loss improved from inf → 0.6944. Saving model...\n",
      "\n",
      "Epoch 2/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:00<00:00, 50.08it/s]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00, 170.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5566 | Val Loss: 0.6748\n",
      "Recall@3: 1.0000 | NDCG@3: 1.0000\n",
      "Recall@5: 1.0000 | NDCG@5: 1.0000\n",
      "✅ Validation loss improved from 0.6944 → 0.6748. Saving model...\n",
      "\n",
      "Epoch 3/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:00<00:00, 52.55it/s]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00, 154.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4394 | Val Loss: 0.6533\n",
      "Recall@3: 1.0000 | NDCG@3: 1.0000\n",
      "Recall@5: 1.0000 | NDCG@5: 1.0000\n",
      "✅ Validation loss improved from 0.6748 → 0.6533. Saving model...\n",
      "\n",
      "Epoch 4/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:00<00:00, 57.00it/s]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00, 144.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3160 | Val Loss: 0.6288\n",
      "Recall@3: 0.0000 | NDCG@3: 0.0000\n",
      "Recall@5: 1.0000 | NDCG@5: 0.4307\n",
      "✅ Validation loss improved from 0.6533 → 0.6288. Saving model...\n",
      "\n",
      "Epoch 5/5\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:00<00:00, 69.03it/s]\n",
      "Validation: 100%|██████████| 1/1 [00:00<00:00, 175.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1971 | Val Loss: 0.6011\n",
      "Recall@3: 0.3333 | NDCG@3: 0.1667\n",
      "Recall@5: 1.0000 | NDCG@5: 0.4538\n",
      "✅ Validation loss improved from 0.6288 → 0.6011. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_two_tower(\n",
    "    model=model,\n",
    "    train_loader=trainloader,      # Your PyTorch DataLoader for training\n",
    "    val_loader=valloader,          # Your PyTorch DataLoader for validation\n",
    "    epochs=5,                      # Number of epochs\n",
    "    lr=1e-3,                        # Learning rate\n",
    "    weight_decay=1e-5,              # L2 regularization\n",
    "    patience=5,                     # Early stopping patience                  # \"cuda\" or \"cpu\"\n",
    "    checkpoint_dir=\"./checkpoints\", # Where to save the best model\n",
    "    checkpoint_name=\"two_tower_best.pt\",\n",
    "    eval_k_list=[3, 5]              # Compute Recall@5, Recall@10, NDCG@5, NDCG@10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53cb2ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No GPU detected\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ GPU available\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5154783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
