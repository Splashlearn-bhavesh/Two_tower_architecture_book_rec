{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5c331f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cloud_storage.redshift_connection import redshift_connection\n",
    "\n",
    "connection = redshift_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e7a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 18:22:06,544 - side - DEBUG - Attempting to read SQL file: sql_files/item_query.sql\n",
      "2025-08-13 18:22:06,546 - side - INFO - Successfully read SQL file: sql_files/item_query.sql\n",
      "2025-08-13 18:22:09,340 - read_shift - INFO - Connected to Redshift successfully.\n",
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/cloud_storage/redshift_connection.py:82: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "2025-08-13 18:23:38,570 - read_shift - INFO - Query executed successfully, retrieved 12189 rows.\n",
      "2025-08-13 18:23:38,570 - read_shift - INFO - Connection closed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>book_isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>authors</th>\n",
       "      <th>book_series</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>rights</th>\n",
       "      <th>illustrators</th>\n",
       "      <th>interactive</th>\n",
       "      <th>search_keywords</th>\n",
       "      <th>...</th>\n",
       "      <th>clicks_students</th>\n",
       "      <th>quality_clicks</th>\n",
       "      <th>quality_clicks_students</th>\n",
       "      <th>students_completed_book</th>\n",
       "      <th>students_completed_75_per_book</th>\n",
       "      <th>per_75_completed_unique_books</th>\n",
       "      <th>completion_rate</th>\n",
       "      <th>time_spent</th>\n",
       "      <th>total_pages</th>\n",
       "      <th>read_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>9780778789208</td>\n",
       "      <td>Baseball in Action</td>\n",
       "      <td>Sarah,Dann</td>\n",
       "      <td>Sports in Action</td>\n",
       "      <td>1999-10-31</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>275.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>52.96</td>\n",
       "      <td>192611.0</td>\n",
       "      <td>11687.0</td>\n",
       "      <td>6408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83</td>\n",
       "      <td>9780778792635</td>\n",
       "      <td>Canada - the people (revised, ed. 3)</td>\n",
       "      <td>Bobbie,Kalman</td>\n",
       "      <td>Lands, Peoples, and Cultures</td>\n",
       "      <td>2009-08-01</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>31.71</td>\n",
       "      <td>2564.0</td>\n",
       "      <td>2201.0</td>\n",
       "      <td>862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173</td>\n",
       "      <td>9781427177261</td>\n",
       "      <td>Explore with Francisco Vazquez de Coronado</td>\n",
       "      <td>Tim,Cooke</td>\n",
       "      <td>Travel with the Great Explorers</td>\n",
       "      <td>2016-08-25</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>222</td>\n",
       "      <td>9780778791263</td>\n",
       "      <td>Friend or Foe?: Plays About Bullying</td>\n",
       "      <td>Catherine,Gourlay</td>\n",
       "      <td>Get into Character</td>\n",
       "      <td>2010-01-15</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>471.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>436.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>53.17</td>\n",
       "      <td>1168356.0</td>\n",
       "      <td>23529.0</td>\n",
       "      <td>13737.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>254</td>\n",
       "      <td>9781039668409</td>\n",
       "      <td>Guide to Werewolves</td>\n",
       "      <td>Carrie,Gleason</td>\n",
       "      <td>Cryptid Guides: Creatures of Folklore</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>World</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>Werewolves, folklore, legends, myths, paranormal</td>\n",
       "      <td>...</td>\n",
       "      <td>4378.0</td>\n",
       "      <td>7144.0</td>\n",
       "      <td>4053.0</td>\n",
       "      <td>2240.0</td>\n",
       "      <td>3879.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>54.61</td>\n",
       "      <td>7666996.0</td>\n",
       "      <td>260700.0</td>\n",
       "      <td>166001.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id      book_isbn                                  book_title  \\\n",
       "0   45  9780778789208                          Baseball in Action   \n",
       "1   83  9780778792635        Canada - the people (revised, ed. 3)   \n",
       "2  173  9781427177261  Explore with Francisco Vazquez de Coronado   \n",
       "3  222  9780778791263        Friend or Foe?: Plays About Bullying   \n",
       "4  254  9781039668409                         Guide to Werewolves   \n",
       "\n",
       "             authors                            book_series publication_date  \\\n",
       "0         Sarah,Dann                       Sports in Action       1999-10-31   \n",
       "1      Bobbie,Kalman           Lands, Peoples, and Cultures       2009-08-01   \n",
       "2          Tim,Cooke        Travel with the Great Explorers       2016-08-25   \n",
       "3  Catherine,Gourlay                     Get into Character       2010-01-15   \n",
       "4     Carrie,Gleason  Cryptid Guides: Creatures of Folklore       2022-09-01   \n",
       "\n",
       "  rights illustrators interactive  \\\n",
       "0  World         None       False   \n",
       "1  World         None       False   \n",
       "2  World         None       False   \n",
       "3  World         None       False   \n",
       "4  World         None       False   \n",
       "\n",
       "                                    search_keywords  ... clicks_students  \\\n",
       "0                                              None  ...           275.0   \n",
       "1                                              None  ...            49.0   \n",
       "2                                              None  ...             7.0   \n",
       "3                                              None  ...           471.0   \n",
       "4  Werewolves, folklore, legends, myths, paranormal  ...          4378.0   \n",
       "\n",
       "  quality_clicks quality_clicks_students students_completed_book  \\\n",
       "0          317.0                   248.0                   134.0   \n",
       "1           49.0                    41.0                    13.0   \n",
       "2            1.0                     1.0                     1.0   \n",
       "3          615.0                   436.0                   235.0   \n",
       "4         7144.0                  4053.0                  2240.0   \n",
       "\n",
       "  students_completed_75_per_book per_75_completed_unique_books  \\\n",
       "0                          226.0                          89.0   \n",
       "1                           37.0                          90.0   \n",
       "2                            1.0                         100.0   \n",
       "3                          410.0                          92.0   \n",
       "4                         3879.0                          94.0   \n",
       "\n",
       "  completion_rate time_spent  total_pages read_pages  \n",
       "0           52.96   192611.0      11687.0     6408.0  \n",
       "1           31.71     2564.0       2201.0      862.0  \n",
       "2          100.00       11.0        248.0       31.0  \n",
       "3           53.17  1168356.0      23529.0    13737.0  \n",
       "4           54.61  7666996.0     260700.0   166001.0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_query = 'sql_files/item_query.sql'\n",
    "book_df = connection.redshift_query_fetching_as_df(item_query)\n",
    "book_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d2d0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tta/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.components.data_transformation import book_data_transformation,user_data_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11ba5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/components/data_transformation.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  book_df['long_description'].fillna('unk',inplace=True)\n",
      "Batches:   0%|          | 0/381 [00:00<?, ?it/s]/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [00:07<00:00, 53.07it/s]\n",
      "Batches:   0%|          | 0/381 [00:00<?, ?it/s]/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [00:36<00:00, 10.39it/s]\n",
      "Batches:   0%|          | 0/381 [00:00<?, ?it/s]/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 381/381 [00:30<00:00, 12.33it/s]\n",
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/components/data_transformation.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved and merged into feature_mappings/theme_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/category_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/reading_skill_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/grades_to_idx.json\n",
      "Dictionary saved and merged into feature_mappings/book_code_to_idx.json\n"
     ]
    }
   ],
   "source": [
    "item_df,  book_feature_count, emb_count =  book_data_transformation(book_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbae67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from src.utils.main_utils import save_dict_to_json\n",
    "\n",
    "# log_dir = 'logs'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# logger = logging.getLogger('data_transformation')\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# console_handler = logging.StreamHandler()\n",
    "# console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# file_handler = logging.FileHandler(os.path.join(log_dir, 'data_transformation.log'), mode='a')\n",
    "# file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# console_handler.setFormatter(formatter)\n",
    "# file_handler.setFormatter(formatter)\n",
    "\n",
    "# logger.addHandler(console_handler)\n",
    "# logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def encode_column_with_sentence_transformer(df: pd.DataFrame, column: str, model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Encodes a column of text into embeddings using a sentence-transformer model.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input dataframe.\n",
    "#         column (str): Column name to encode.\n",
    "#         model_name (str): Pretrained sentence-transformers model name.\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Array of shape (num_rows, embedding_dim)\n",
    "#     \"\"\"\n",
    "#     model = SentenceTransformer(model_name)\n",
    "    \n",
    "#     # Fill missing values\n",
    "#     texts = df[column].fillna(\"unk\").astype(str).tolist()\n",
    "    \n",
    "#     # Encode with model\n",
    "#     embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "#     return np.array(embeddings)\n",
    "\n",
    "# def pre_train_emb_creation(book_df):\n",
    "\n",
    "#     book_df['title_plus_author'] = book_df.apply(lambda x:x['book_title'].lower()+' by '+x['authors'].lower(),axis=1)\n",
    "#     book_df['long_description'].fillna('unk',inplace=True)\n",
    "#     book_df['long_description'] = book_df.apply(lambda x:x['long_description'].lower(),axis=1)\n",
    "#     columns = ['book_isbn', 'title_plus_author', 'book_series', 'book_type', 'long_description','min_grade', 'max_grade',\n",
    "#         'readable_page_count','fiction_nonfiction', 'reading_skill_name','theme_name', 'category_name','language_book']\n",
    "\n",
    "#     book_df_final = book_df[columns]\n",
    "\n",
    "#     emb = encode_column_with_sentence_transformer(book_df_final,'title_plus_author')\n",
    "#     emb_df = pd.DataFrame(emb, columns=[f\"emb_title_author_{i}\" for i in range(emb.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     book_embedding_author_df = pd.concat([book_df_final, emb_df], axis=1)\n",
    "\n",
    "#     emb_desc = encode_column_with_sentence_transformer(book_embedding_author_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_desc_df = pd.DataFrame(emb_desc, columns=[f\"emb_desc_{i}\" for i in range(emb_desc.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     long_description_df = pd.concat([book_embedding_author_df, emb_desc_df], axis=1)\n",
    "#     emb_book_series = encode_column_with_sentence_transformer(long_description_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_book_series_df = pd.DataFrame(emb_book_series, columns=[f\"emb_book_series_{i}\" for i in range(emb_book_series.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "\n",
    "#     book_series_df = pd.concat([long_description_df,emb_book_series_df ], axis=1)\n",
    "\n",
    "#     return book_series_df , emb.shape[1] ,emb_desc.shape[1] ,emb_book_series.shape[1]\n",
    "\n",
    "# def clip(df,col,min,max):\n",
    "#     df[col] = np.clip(df[col],min,max)\n",
    "#     return df \n",
    "\n",
    "# def scaling(df, col ,value):\n",
    "#     df[col] = df[col]/value\n",
    "#     return df\n",
    "\n",
    "# grade_list = ['pk', 'k', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "# grade_to_idx = {g: i for i, g in enumerate(grade_list)}\n",
    "\n",
    "# def get_range(min_g, max_g):\n",
    "#     start_idx = grade_to_idx[min_g]\n",
    "#     end_idx = grade_to_idx[max_g]\n",
    "#     return ','.join(grade_list[start_idx:end_idx + 1])\n",
    "\n",
    "# def get_category_mapping(item_df, input_col, out_put_col,json_file_name):\n",
    "\n",
    "#     # Step 1: Preprocess themes (split on commas)\n",
    "#     item_df['themes'] = item_df[input_col].fillna('').apply(\n",
    "#         lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "#     )\n",
    "#     # Step 2: Build theme vocabulary\n",
    "#     from itertools import chain\n",
    "#     all_themes = sorted(set(chain.from_iterable(item_df['themes'])))\n",
    "#     theme_to_idx = {theme: idx for idx, theme in enumerate(all_themes)}\n",
    "#     if 'unk' not in theme_to_idx:\n",
    "#         theme_to_idx['unk'] = len(theme_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df['themes'].apply(\n",
    "#         lambda theme_list: [theme_to_idx[t] for t in theme_list if t in theme_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(theme_to_idx,json_file_name)\n",
    "#     return item_df, len(theme_to_idx)\n",
    "\n",
    "# def get_category_mapping_book(item_df,input_col ,out_put_col,json_file_name):\n",
    "\n",
    "#     book_code_to_idx = {theme: idx for idx, theme in enumerate(list((item_df[input_col])))}\n",
    "#     if 'unk' not in book_code_to_idx:\n",
    "#         book_code_to_idx['unk'] = len(book_code_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df[input_col].apply(\n",
    "#         lambda theme_list: [book_code_to_idx[t] for t in [theme_list] if t in book_code_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(book_code_to_idx,json_file_name)\n",
    "#     return item_df, len(book_code_to_idx)\n",
    "\n",
    "\n",
    "# # book_df_final['readable_page_count'] = np.clip(book_df_final['readable_page_count'],0,50)/50\n",
    "# # book_series_df.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def book_data_transformation(book_df):\n",
    "\n",
    "#     book_series_df, emb_shape,emb_desc_shape,emb_book_series_shape = pre_train_emb_creation(book_df)\n",
    "\n",
    "#     book_series_df = scaling(clip(book_series_df,'readable_page_count',0,50),'readable_page_count',50)\n",
    "\n",
    "#     book_series_df['book_type_binary'] = np.where(book_series_df.book_type == 'PDF',1,0)\n",
    "\n",
    "#     book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n",
    "#     book_df_final_v1 = pd.get_dummies(book_series_df, columns=['fiction_nonfiction'], prefix='fn')\n",
    "\n",
    "#     book_df_final_v1 = pd.get_dummies(book_df_final_v1, columns=['language_book'], prefix='lang')\n",
    "\n",
    "#     book_df_final_v1[\"grades\"] = book_df_final_v1.apply(\n",
    "#         lambda row: get_range(row[\"min_grade\"], row[\"max_grade\"]),axis=1)\n",
    "\n",
    "#     book_df_final_v1, theme_count = get_category_mapping(book_df_final_v1, 'theme_name', 'theme_ids','theme_to_idx.json')\n",
    "#     book_df_final_v1, category_count = get_category_mapping(book_df_final_v1, 'category_name', 'category_ids','category_to_idx.json')\n",
    "#     book_df_final_v1, reading_skills_count = get_category_mapping(book_df_final_v1, 'reading_skill_name', 'reading_skill_ids','reading_skill_to_idx.json')\n",
    "#     book_df_final_v1, grades_count = get_category_mapping(book_df_final_v1, 'grades', 'grades_ids','grades_to_idx.json')\n",
    "#     book_df_final_v1, book_count = get_category_mapping_book(book_df_final_v1,'book_isbn' ,'book_code_ids','book_code_to_idx.json')\n",
    "#     book_df_final_v1.rename(columns={'book_isbn':'book_code'},inplace=True)\n",
    "#     book_feature_count =  {\n",
    "#                     'themes_count':theme_count, \n",
    "#                     'book_count': book_count, \n",
    "#                     'grade_count': grades_count,\n",
    "#                     'reading_skills_count':reading_skills_count,\n",
    "#                     'category_count':category_count\n",
    "#                    }\n",
    "#     emb_count = {\n",
    "#                     'themes_count':8, \n",
    "#                     'book_count': 16, \n",
    "#                     'grade_count': 4,\n",
    "#                     'reading_skills_count':4,\n",
    "#                     'category_count':4\n",
    "#                 }\n",
    "    \n",
    "#     columns_author_title =[f\"emb_title_author_{i}\" for i in range(emb_shape)]\n",
    "#     columns_long_description = [f\"emb_desc_{i}\" for i in range(emb_desc_shape)]\n",
    "#     columns_book_series = [f\"emb_book_series_{i}\" for i in range(emb_book_series_shape)]\n",
    "#     columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "#         'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "#         'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "#     columns_learn_emb = ['book_code','book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "#     book_feature_cols = columns_learn_emb + columns_author_title + columns_long_description + columns_book_series + columns_add  \n",
    "\n",
    "#     return book_df_final_v1[book_feature_cols], book_feature_count, emb_count \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e7e28a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_code', 'book_code_ids', 'grades_ids', 'reading_skill_ids',\n",
       "       'category_ids', 'theme_ids', 'emb_title_author_0', 'emb_title_author_1',\n",
       "       'emb_title_author_2', 'emb_title_author_3',\n",
       "       ...\n",
       "       'book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
       "       'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
       "       'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish'],\n",
       "      dtype='object', length=1169)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7813b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import logging\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from src.utils.main_utils import save_dict_to_json,  load_json_file\n",
    "\n",
    "\n",
    "# log_dir = 'logs'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# logger = logging.getLogger('data_transformation')\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "# console_handler = logging.StreamHandler()\n",
    "# console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# file_handler = logging.FileHandler(os.path.join(log_dir, 'data_transformation.log'), mode='a')\n",
    "# file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# console_handler.setFormatter(formatter)\n",
    "# file_handler.setFormatter(formatter)\n",
    "\n",
    "# logger.addHandler(console_handler)\n",
    "# logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def encode_column_with_sentence_transformer(df: pd.DataFrame, column: str, model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Encodes a column of text into embeddings using a sentence-transformer model.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input dataframe.\n",
    "#         column (str): Column name to encode.\n",
    "#         model_name (str): Pretrained sentence-transformers model name.\n",
    "\n",
    "#     Returns:\n",
    "#         np.ndarray: Array of shape (num_rows, embedding_dim)\n",
    "#     \"\"\"\n",
    "#     model = SentenceTransformer(model_name)\n",
    "    \n",
    "#     # Fill missing values\n",
    "#     texts = df[column].fillna(\"unk\").astype(str).tolist()\n",
    "    \n",
    "#     # Encode with model\n",
    "#     embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "#     return np.array(embeddings)\n",
    "\n",
    "# def pre_train_emb_creation(book_df):\n",
    "\n",
    "#     book_df['title_plus_author'] = book_df.apply(lambda x:x['book_title'].lower()+' by '+x['authors'].lower(),axis=1)\n",
    "#     book_df['long_description'].fillna('unk',inplace=True)\n",
    "#     book_df['long_description'] = book_df.apply(lambda x:x['long_description'].lower(),axis=1)\n",
    "#     columns = ['book_isbn', 'title_plus_author', 'book_series', 'book_type', 'long_description','min_grade', 'max_grade',\n",
    "#         'readable_page_count','fiction_nonfiction', 'reading_skill_name','theme_name', 'category_name','language_book']\n",
    "\n",
    "#     book_df_final = book_df[columns]\n",
    "\n",
    "#     emb = encode_column_with_sentence_transformer(book_df_final,'title_plus_author')\n",
    "#     emb_df = pd.DataFrame(emb, columns=[f\"emb_title_author_{i}\" for i in range(emb.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     book_embedding_author_df = pd.concat([book_df_final, emb_df], axis=1)\n",
    "\n",
    "#     emb_desc = encode_column_with_sentence_transformer(book_embedding_author_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_desc_df = pd.DataFrame(emb_desc, columns=[f\"emb_desc_{i}\" for i in range(emb_desc.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "#     long_description_df = pd.concat([book_embedding_author_df, emb_desc_df], axis=1)\n",
    "#     emb_book_series = encode_column_with_sentence_transformer(long_description_df,'long_description')\n",
    "#     # Convert embeddings to DataFrame\n",
    "#     emb_book_series_df = pd.DataFrame(emb_book_series, columns=[f\"emb_book_series_{i}\" for i in range(emb_book_series.shape[1])])\n",
    "\n",
    "#     # Combine with book_id\n",
    "\n",
    "#     book_series_df = pd.concat([long_description_df,emb_book_series_df ], axis=1)\n",
    "\n",
    "#     return book_series_df , emb.shape[1] ,emb_desc.shape[1] ,emb_book_series.shape[1]\n",
    "\n",
    "# def clip(df,col,min,max):\n",
    "#     df[col] = np.clip(df[col],min,max)\n",
    "#     return df \n",
    "\n",
    "# def scaling(df, col ,value):\n",
    "#     df[col] = df[col]/value\n",
    "#     return df\n",
    "\n",
    "# grade_list = ['pk', 'k', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "# grade_to_idx = {g: i for i, g in enumerate(grade_list)}\n",
    "\n",
    "# def get_range(min_g, max_g):\n",
    "#     start_idx = grade_to_idx[min_g]\n",
    "#     end_idx = grade_to_idx[max_g]\n",
    "#     return ','.join(grade_list[start_idx:end_idx + 1])\n",
    "\n",
    "# def get_category_mapping(item_df, input_col, out_put_col,json_file_name):\n",
    "\n",
    "#     # Step 1: Preprocess themes (split on commas)\n",
    "#     item_df['themes'] = item_df[input_col].fillna('').apply(\n",
    "#         lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "#     )\n",
    "#     # Step 2: Build theme vocabulary\n",
    "#     from itertools import chain\n",
    "#     all_themes = sorted(set(chain.from_iterable(item_df['themes'])))\n",
    "#     theme_to_idx = {theme: idx for idx, theme in enumerate(all_themes)}\n",
    "#     if 'unk' not in theme_to_idx:\n",
    "#         theme_to_idx['unk'] = len(theme_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df['themes'].apply(\n",
    "#         lambda theme_list: [theme_to_idx[t] for t in theme_list if t in theme_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(theme_to_idx,json_file_name)\n",
    "#     return item_df, len(theme_to_idx)\n",
    "\n",
    "# def get_category_mapping_book(item_df,input_col ,out_put_col,json_file_name):\n",
    "\n",
    "#     book_code_to_idx = {theme: idx for idx, theme in enumerate(list((item_df[input_col])))}\n",
    "#     if 'unk' not in book_code_to_idx:\n",
    "#         book_code_to_idx['unk'] = len(book_code_to_idx)\n",
    "\n",
    "#     # Step 3: Map themes to indices\n",
    "#     item_df[out_put_col] = item_df[input_col].apply(\n",
    "#         lambda theme_list: [book_code_to_idx[t] for t in [theme_list] if t in book_code_to_idx]\n",
    "#     )\n",
    "#     save_dict_to_json(book_code_to_idx,json_file_name)\n",
    "#     return item_df, len(book_code_to_idx)\n",
    "\n",
    "\n",
    "# # book_df_final['readable_page_count'] = np.clip(book_df_final['readable_page_count'],0,50)/50\n",
    "# # book_series_df.shape\n",
    "\n",
    "# def last_10_books_fast(df):\n",
    "#     df = df.copy()\n",
    "#     df['book_create_dt'] = pd.to_datetime(df['book_create_dt'])\n",
    "#     df = df.sort_values(['user_id', 'book_create_dt']).reset_index(drop=True)\n",
    "\n",
    "#     # Helper to join last 10 values for each row in a group\n",
    "#     def last_10_join(series):\n",
    "#         out = []\n",
    "#         hist = []\n",
    "#         for val in series:\n",
    "#             out.append(','.join(hist[-10:]) if hist else 'unk')\n",
    "#             hist.append(val)\n",
    "#         return pd.Series(out, index=series.index)\n",
    "\n",
    "#     # Precompute category/theme strings\n",
    "#     # df['cat_str'] = df['category_name'].apply(lambda x: ','.join(x))\n",
    "#     # print(df['cat_str'])\n",
    "#     # df['theme_str'] = df['theme_name'].apply(lambda x: ','.join(x))\n",
    "#     df['cat_str'] = df['category_name']\n",
    "#     # print(df['cat_str'])\n",
    "#     df['theme_str'] = df['theme_name']\n",
    "#     df['rs_str'] = df['reading_skill_name']\n",
    "\n",
    "\n",
    "#     # Vectorized per-group computation (one Python loop per group, not per row globally)\n",
    "#     grouped = df.groupby('user_id', group_keys=False)\n",
    "#     df['last_10_books'] = grouped['book_code'].transform(last_10_join)\n",
    "#     df['last_category_name'] = grouped['cat_str'].transform(last_10_join)\n",
    "#     df['last_theme_name'] = grouped['theme_str'].transform(last_10_join)\n",
    "#     df['last_reading_skill_name'] = grouped['rs_str'].transform(last_10_join)\n",
    "\n",
    "#     return df.drop(columns=['cat_str', 'theme_str','rs_str'])\n",
    "\n",
    "# def get_mapping_user(child_df , input_col, output_col , file_path ):\n",
    "#     category_to_idx = load_json_file(file_path)\n",
    "#     child_df['last_categories'] = child_df[input_col].fillna('').apply(\n",
    "#     lambda x: [t.strip().lower() for t in x.split(',') if t.strip()])\n",
    "#     child_df[output_col] = child_df['last_categories'].apply(\n",
    "#     lambda theme_list: [category_to_idx[t] for t in theme_list if t in category_to_idx] )\n",
    "\n",
    "#     return child_df, len(category_to_idx)\n",
    "\n",
    "# def get_mapping_book_user(child_df , input_col, output_col , file_path ):\n",
    "#     book_code_to_idx = load_json_file(file_path)\n",
    "#     child_df['last_books_list'] = child_df[input_col].fillna('').apply(\n",
    "#     lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "# )\n",
    "\n",
    "#     child_df[output_col] = child_df['last_books_list'].apply(\n",
    "#     lambda theme_list: [book_code_to_idx[int(t)] if t!='unk' else book_code_to_idx[t]  for t in theme_list ])\n",
    "\n",
    "#     return child_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def book_data_transformation(book_df):\n",
    "\n",
    "#     book_series_df, emb_shape,emb_desc_shape,emb_book_series_shape = pre_train_emb_creation(book_df)\n",
    "\n",
    "#     book_series_df = scaling(clip(book_series_df,'readable_page_count',0,50),'readable_page_count',50)\n",
    "\n",
    "#     book_series_df['book_type_binary'] = np.where(book_series_df.book_type == 'PDF',1,0)\n",
    "\n",
    "#     book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n",
    "#     book_df_final_v1 = pd.get_dummies(book_series_df, columns=['fiction_nonfiction'], prefix='fn')\n",
    "\n",
    "#     book_df_final_v1 = pd.get_dummies(book_df_final_v1, columns=['language_book'], prefix='lang')\n",
    "\n",
    "#     book_df_final_v1[\"grades\"] = book_df_final_v1.apply(\n",
    "#         lambda row: get_range(row[\"min_grade\"], row[\"max_grade\"]),axis=1)\n",
    "\n",
    "#     book_df_final_v1, theme_count = get_category_mapping(book_df_final_v1, 'theme_name', 'theme_ids','theme_to_idx.json')\n",
    "#     book_df_final_v1, category_count = get_category_mapping(book_df_final_v1, 'category_name', 'category_ids','category_to_idx.json')\n",
    "#     book_df_final_v1, reading_skills_count = get_category_mapping(book_df_final_v1, 'reading_skill_name', 'reading_skill_ids','reading_skill_to_idx.json')\n",
    "#     book_df_final_v1, grades_count = get_category_mapping(book_df_final_v1, 'grades', 'grades_ids','grades_to_idx.json')\n",
    "#     book_df_final_v1, book_count = get_category_mapping_book(book_df_final_v1,'book_isbn' ,'book_code_ids','book_code_to_idx.json')\n",
    "#     book_df_final_v1.rename(columns={'book_isbn':'book_code'},inplace=True)\n",
    "#     book_feature_count =  {\n",
    "#                     'themes_count':theme_count, \n",
    "#                     'book_count': book_count, \n",
    "#                     'grade_count': grades_count,\n",
    "#                     'reading_skills_count':reading_skills_count,\n",
    "#                     'category_count':category_count\n",
    "#                    }\n",
    "#     emb_count = {\n",
    "#                     'themes_count':8, \n",
    "#                     'book_count': 16, \n",
    "#                     'grade_count': 4,\n",
    "#                     'reading_skills_count':4,\n",
    "#                     'category_count':4\n",
    "#                 }\n",
    "    \n",
    "#     columns_author_title =[f\"emb_title_author_{i}\" for i in range(emb_shape)]\n",
    "#     columns_long_description = [f\"emb_desc_{i}\" for i in range(emb_desc_shape)]\n",
    "#     columns_book_series = [f\"emb_book_series_{i}\" for i in range(emb_book_series_shape)]\n",
    "#     columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "#         'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "#         'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "#     columns_learn_emb = ['book_code','book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "#     book_feature_cols = columns_learn_emb + columns_author_title + columns_long_description + columns_book_series + columns_add  \n",
    "\n",
    "#     return book_df_final_v1[book_feature_cols], book_feature_count, emb_count \n",
    "\n",
    "\n",
    "# def user_data_transformation(user_df,user_loc,user_platform): \n",
    "#     # user_platform.rename(columns ={'isbn':'book_code'},inplace=True)\n",
    "#     df1 = user_platform.loc[:, ~user_platform.columns.duplicated()]\n",
    "#     user_platform = df1[['user_id', 'book_code', 'book_create_dt',\n",
    "#        'cumulative_web_during_school_hour',\n",
    "#        'cumulative_web_after_school_hour',\n",
    "#        'cumulative_apple_during_school_hour',\n",
    "#        'cumulative_apple_after_school_hour',\n",
    "#        'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "#        'cumulative_unk_during_school_hour',\n",
    "#        'cumulative_unk_after_school_hour']]\n",
    "    \n",
    "#     user_platform['total'] = user_platform['cumulative_web_during_school_hour'] + user_platform['cumulative_web_after_school_hour'] + user_platform['cumulative_apple_during_school_hour']+ user_platform['cumulative_apple_after_school_hour']+ user_platform['cumulative_android_during_school_hour']+ user_platform['cumulative_android_after_school_hour']+ user_platform['cumulative_unk_during_school_hour']+ user_platform['cumulative_unk_after_school_hour']\n",
    "\n",
    "#     user_platform['cumulative_web_during_school_hour'] = user_platform['cumulative_web_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_web_after_school_hour']  = user_platform['cumulative_web_after_school_hour'] /user_platform['total']\n",
    "#     user_platform['cumulative_apple_during_school_hour'] = user_platform['cumulative_apple_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_apple_after_school_hour'] = user_platform['cumulative_apple_after_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_android_during_school_hour'] = user_platform['cumulative_android_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_android_after_school_hour'] = user_platform['cumulative_android_after_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_unk_during_school_hour'] = user_platform['cumulative_unk_during_school_hour']/user_platform['total']\n",
    "#     user_platform['cumulative_unk_after_school_hour']  = user_platform['cumulative_unk_after_school_hour']/user_platform['total']\n",
    "    \n",
    "#     user_platform_final = user_platform[['user_id', 'book_code',\n",
    "#        'cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "#        'cumulative_apple_during_school_hour',\n",
    "#        'cumulative_apple_after_school_hour',\n",
    "#        'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "#        'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "#        ]]\n",
    "\n",
    "#     user_df.dropna(subset=['category_name'],inplace=True)\n",
    "\n",
    "\n",
    "#     user_df['category_name'] = user_df['category_name'].fillna('unk')\n",
    "#     user_df['theme_name'] = user_df['theme_name'].fillna('unk')\n",
    "#     user_df['reading_skill_name'] = user_df['reading_skill_name'].fillna('unk')\n",
    "\n",
    "#     user_df['total_pages']=user_df['total_pages'].fillna(user_df['total_pages'].median())\n",
    "#     user_df['max_read_pages']=user_df['max_read_pages'].fillna(user_df['max_read_pages'].median())\n",
    "\n",
    "\n",
    "#     user_df_v1 = user_df[['book_code', 'user_id','category_name','theme_name','reading_skill_name', 'book_create_dt', 'total_pages',\n",
    "#        'max_read_pages']].copy()\n",
    "\n",
    "#     user_df_v1['book_create_dt'] = pd.to_datetime(user_df_v1['book_create_dt'])\n",
    "\n",
    "#     user_loc_v1 = user_loc[['user_id','country', 'state', 'zipcode','klass_grade_name','teacher_id','school_id','class_activation_bucket']].copy()\n",
    "    \n",
    "#     user_raw_df =  user_df_v1.merge(user_loc_v1, how ='left' ,on = 'user_id')\n",
    "\n",
    "#     user_platform_final['book_code'] = user_platform_final['book_code'].astype('str')\n",
    "\n",
    "#     user_raw_df_v1 = user_raw_df.merge(user_platform_final, how ='left' ,on = ['user_id','book_code'])\n",
    "\n",
    "#     cv = user_raw_df_v1 .copy()\n",
    "\n",
    "#     user_raw_df_v2 = last_10_books_fast(cv)\n",
    "#     user_raw_df_v2['class_activation_bucket'] = user_raw_df_v2['class_activation_bucket'].fillna('unk')\n",
    "#     user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['klass_grade_name'], prefix='grade')\n",
    "#     user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['class_activation_bucket'], prefix='class_activation_bucket')\n",
    "\n",
    "#     user_raw_df_v2['completion_rate'] = user_raw_df_v2['max_read_pages']/user_raw_df_v2['total_pages']\n",
    "#     user_raw_df_v2['label'] = np.where(user_raw_df_v2['completion_rate']>0.5,1,0)\n",
    "\n",
    "#     user_raw_df_v2, category_count  = get_mapping_user(user_raw_df_v2 , 'last_category_name', 'category_ids' , 'feature_mappings/category_to_idx.json')\n",
    "#     user_raw_df_v2, book_count = get_mapping_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "#     # user_raw_df_v2, book_count = get_mapping_book_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "#     user_raw_df_v2, reading_skills_count  = get_mapping_user(user_raw_df_v2 , 'last_reading_skill_name', 'reading_skill_ids' , 'feature_mappings/reading_skill_to_idx.json')\n",
    "#     user_raw_df_v2, theme_count = get_mapping_user(user_raw_df_v2 , 'last_theme_name', 'theme_ids' , 'feature_mappings/theme_to_idx.json')\n",
    "#     user_raw_df_v2, country_count = get_category_mapping(user_raw_df_v2, 'country', 'countries_ids','country_to_idx.json')\n",
    "#     user_raw_df_v2, state_count = get_category_mapping(user_raw_df_v2, 'state', 'states_ids','state_to_idx.json')\n",
    "#     user_raw_df_v2, zipcode_count = get_category_mapping(user_raw_df_v2, 'zipcode', 'zipcode_ids','zipcode_to_idx.json')\n",
    "#     user_raw_df_v2, teacher_count = get_category_mapping(user_raw_df_v2, 'teacher_id', 'teacher_code_ids','teacher_to_idx.json')\n",
    "#     user_raw_df_v2, school_count = get_category_mapping(user_raw_df_v2, 'school_id', 'school_code_ids','school_to_idx.json')\n",
    "\n",
    "#     user_columns = ['book_code', 'user_id', 'book_create_dt','book_code_ids','category_ids', 'state', 'zipcode',\n",
    "#        'teacher_id', 'school_id','cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "#        'cumulative_apple_during_school_hour',\n",
    "#        'cumulative_apple_after_school_hour',\n",
    "#        'cumulative_android_during_school_hour',\n",
    "#        'cumulative_android_after_school_hour',\n",
    "#        'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "#         'grade_grade 1', 'grade_grade 2', 'grade_grade 3',\n",
    "#        'grade_grade 4', 'grade_grade 5', 'grade_kindergarten', \n",
    "#        'class_activation_bucket_AC', 'class_activation_bucket_AC0',\n",
    "#        'class_activation_bucket_AC1', 'class_activation_bucket_AC2',\n",
    "#        'class_activation_bucket_AC3', 'class_activation_bucket_unk', 'last_10_books', 'last_category_name', 'last_theme_name',\n",
    "#        'last_reading_skill_name','label']\n",
    "\n",
    "#     user_feature_count =  {'themes_count':theme_count, \n",
    "#                    'book_count': book_count, \n",
    "#                    'reading_skills_count':reading_skills_count,\n",
    "#                    'category_count':category_count,\n",
    "#                    'country_count': country_count , \n",
    "#                     'state_count': state_count ,\n",
    "#                     'zipcode_count': zipcode_count,\n",
    "#                     'teacher_count': teacher_count,\n",
    "#                     'school_count': school_count\n",
    "#                    }\n",
    "#     user_emb_count = {\n",
    "#                 'themes_count':8, \n",
    "#                 'book_count': 16, \n",
    "#                 'reading_skills_count':4,\n",
    "#                 'category_count':4,\n",
    "#                 'country_count': 8 , \n",
    "#                 'state_count': 10,\n",
    "#                 'zipcode_count': 14,\n",
    "#                 'teacher_count': 16,\n",
    "#                 'school_count': 16\n",
    "#                 }\n",
    "\n",
    "#     return user_raw_df_v2[user_columns], user_feature_count, user_emb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a6da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 18:33:31,211 - side - DEBUG - Attempting to read SQL file: sql_files/user_book_platform.sql\n",
      "2025-08-13 18:33:31,215 - side - INFO - Successfully read SQL file: sql_files/user_book_platform.sql\n",
      "2025-08-13 18:33:34,123 - read_shift - INFO - Connected to Redshift successfully.\n",
      "/Users/bhaveshraj/recommendation_system/Two-Tower/src/cloud_storage/redshift_connection.py:82: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "2025-08-13 18:46:28,694 - read_shift - INFO - Query executed successfully, retrieved 4963473 rows.\n",
      "2025-08-13 18:46:28,697 - read_shift - INFO - Connection closed.\n",
      "2025-08-13 18:46:28,738 - side - DEBUG - Attempting to read SQL file: sql_files/user_location.sql\n",
      "2025-08-13 18:46:28,739 - side - INFO - Successfully read SQL file: sql_files/user_location.sql\n",
      "2025-08-13 18:46:31,531 - read_shift - INFO - Connected to Redshift successfully.\n",
      "2025-08-13 18:48:29,669 - read_shift - INFO - Query executed successfully, retrieved 662272 rows.\n",
      "2025-08-13 18:48:29,671 - read_shift - INFO - Connection closed.\n",
      "2025-08-13 18:48:29,671 - side - DEBUG - Attempting to read SQL file: sql_files/user_query.sql\n",
      "2025-08-13 18:48:29,672 - side - INFO - Successfully read SQL file: sql_files/user_query.sql\n",
      "2025-08-13 18:48:32,362 - read_shift - INFO - Connected to Redshift successfully.\n",
      "2025-08-13 18:54:04,623 - read_shift - INFO - Query executed successfully, retrieved 4963473 rows.\n",
      "2025-08-13 18:54:04,627 - read_shift - INFO - Connection closed.\n"
     ]
    }
   ],
   "source": [
    "platfrom_query = 'sql_files/user_book_platform.sql'\n",
    "user_platform = connection.redshift_query_fetching_as_df(platfrom_query)\n",
    "location_query = 'sql_files/user_location.sql'\n",
    "user_loc = connection.redshift_query_fetching_as_df(location_query)\n",
    "user_query = 'sql_files/user_query.sql'\n",
    "user_df = connection.redshift_query_fetching_as_df(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "996c6989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4963473 entries, 0 to 4963472\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   book_code           object        \n",
      " 1   user_id             object        \n",
      " 2   book_create_dt      datetime64[ns]\n",
      " 3   total_pages         float64       \n",
      " 4   max_read_pages      float64       \n",
      " 5   latest_to_old_rank  int64         \n",
      " 6   theme_name          object        \n",
      " 7   category_name       object        \n",
      " 8   reading_skill_name  object        \n",
      " 9   language_book       object        \n",
      " 10  book_series         object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(7)\n",
      "memory usage: 416.6+ MB\n"
     ]
    }
   ],
   "source": [
    "user_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34820abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4963473 entries, 0 to 4963472\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                 Dtype \n",
      "---  ------                                 ----- \n",
      " 0   user_id                                object\n",
      " 1   book_code                              object\n",
      " 2   book_create_dt                         object\n",
      " 3   cumulative_web_during_school_hour      int64 \n",
      " 4   cumulative_web_after_school_hour       int64 \n",
      " 5   cumulative_apple_during_school_hour    int64 \n",
      " 6   cumulative_apple_after_school_hour     int64 \n",
      " 7   cumulative_android_during_school_hour  int64 \n",
      " 8   cumulative_android_after_school_hour   int64 \n",
      " 9   cumulative_unk_during_school_hour      int64 \n",
      " 10  cumulative_unk_after_school_hour       int64 \n",
      "dtypes: int64(8), object(3)\n",
      "memory usage: 416.6+ MB\n"
     ]
    }
   ],
   "source": [
    "user_platform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5694368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 662272 entries, 0 to 662271\n",
      "Data columns (total 18 columns):\n",
      " #   Column                      Non-Null Count   Dtype         \n",
      "---  ------                      --------------   -----         \n",
      " 0   user_id                     662272 non-null  object        \n",
      " 1   teacher_creation_source     662272 non-null  object        \n",
      " 2   country                     661821 non-null  object        \n",
      " 3   state                       637458 non-null  object        \n",
      " 4   zipcode                     630262 non-null  object        \n",
      " 5   klass_grade_name            662272 non-null  object        \n",
      " 6   klass_id                    662272 non-null  object        \n",
      " 7   classroom_type              662027 non-null  object        \n",
      " 8   teacher_id                  662272 non-null  object        \n",
      " 9   teacher_create_dt           662272 non-null  datetime64[ns]\n",
      " 10  teacher_create_school_year  662272 non-null  float64       \n",
      " 11  school_id                   662203 non-null  object        \n",
      " 12  ac3                         534252 non-null  object        \n",
      " 13  ac2                         641602 non-null  datetime64[ns]\n",
      " 14  ac                          649942 non-null  datetime64[ns]\n",
      " 15  ac1                         658659 non-null  datetime64[ns]\n",
      " 16  ac0                         660622 non-null  datetime64[ns]\n",
      " 17  class_activation_bucket     660667 non-null  object        \n",
      "dtypes: datetime64[ns](5), float64(1), object(12)\n",
      "memory usage: 90.9+ MB\n"
     ]
    }
   ],
   "source": [
    "user_loc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2978270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa02f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_df.to_csv('user_interaction_data.csv',index=False)\n",
    "# user_loc.to_csv('user_location_data.csv',index=False)\n",
    "# user_platform.to_csv('user_platform_data.csv',index=False)\n",
    "user_df= pd.read_csv('user_interaction_data.csv')\n",
    "user_loc =pd.read_csv('user_location_data.csv')\n",
    "user_platform = pd.read_csv('user_platform_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013b4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4963473 entries, 0 to 4963472\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                 Dtype \n",
      "---  ------                                 ----- \n",
      " 0   user_id                                object\n",
      " 1   book_code                              int64 \n",
      " 2   book_create_dt                         object\n",
      " 3   cumulative_web_during_school_hour      int64 \n",
      " 4   cumulative_web_after_school_hour       int64 \n",
      " 5   cumulative_apple_during_school_hour    int64 \n",
      " 6   cumulative_apple_after_school_hour     int64 \n",
      " 7   cumulative_android_during_school_hour  int64 \n",
      " 8   cumulative_android_after_school_hour   int64 \n",
      " 9   cumulative_unk_during_school_hour      int64 \n",
      " 10  cumulative_unk_after_school_hour       int64 \n",
      "dtypes: int64(9), object(2)\n",
      "memory usage: 416.6+ MB\n"
     ]
    }
   ],
   "source": [
    "user_platform.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.utils.main_utils import save_dict_to_json,  load_json_file\n",
    "\n",
    "\n",
    "log_dir = 'logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "logger = logging.getLogger('data_transformation')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "file_handler = logging.FileHandler(os.path.join(log_dir, 'data_transformation.log'), mode='a')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_column_with_sentence_transformer(df: pd.DataFrame, column: str, model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encodes a column of text into embeddings using a sentence-transformer model.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe.\n",
    "        column (str): Column name to encode.\n",
    "        model_name (str): Pretrained sentence-transformers model name.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (num_rows, embedding_dim)\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Fill missing values\n",
    "    texts = df[column].fillna(\"unk\").astype(str).tolist()\n",
    "    \n",
    "    # Encode with model\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def pre_train_emb_creation(book_df):\n",
    "\n",
    "    book_df['title_plus_author'] = book_df.apply(lambda x:x['book_title'].lower()+' by '+x['authors'].lower(),axis=1)\n",
    "    book_df['long_description'].fillna('unk',inplace=True)\n",
    "    book_df['long_description'] = book_df.apply(lambda x:x['long_description'].lower(),axis=1)\n",
    "    columns = ['book_isbn', 'title_plus_author', 'book_series', 'book_type', 'long_description','min_grade', 'max_grade',\n",
    "        'readable_page_count','fiction_nonfiction', 'reading_skill_name','theme_name', 'category_name','language_book']\n",
    "\n",
    "    book_df_final = book_df[columns]\n",
    "\n",
    "    emb = encode_column_with_sentence_transformer(book_df_final,'title_plus_author')\n",
    "    emb_df = pd.DataFrame(emb, columns=[f\"emb_title_author_{i}\" for i in range(emb.shape[1])])\n",
    "\n",
    "    # Combine with book_id\n",
    "    book_embedding_author_df = pd.concat([book_df_final, emb_df], axis=1)\n",
    "\n",
    "    emb_desc = encode_column_with_sentence_transformer(book_embedding_author_df,'long_description')\n",
    "    # Convert embeddings to DataFrame\n",
    "    emb_desc_df = pd.DataFrame(emb_desc, columns=[f\"emb_desc_{i}\" for i in range(emb_desc.shape[1])])\n",
    "\n",
    "    # Combine with book_id\n",
    "    long_description_df = pd.concat([book_embedding_author_df, emb_desc_df], axis=1)\n",
    "    emb_book_series = encode_column_with_sentence_transformer(long_description_df,'long_description')\n",
    "    # Convert embeddings to DataFrame\n",
    "    emb_book_series_df = pd.DataFrame(emb_book_series, columns=[f\"emb_book_series_{i}\" for i in range(emb_book_series.shape[1])])\n",
    "\n",
    "    # Combine with book_id\n",
    "\n",
    "    book_series_df = pd.concat([long_description_df,emb_book_series_df ], axis=1)\n",
    "\n",
    "    return book_series_df , emb.shape[1] ,emb_desc.shape[1] ,emb_book_series.shape[1]\n",
    "\n",
    "def clip(df,col,min,max):\n",
    "    df[col] = np.clip(df[col],min,max)\n",
    "    return df \n",
    "\n",
    "def scaling(df, col ,value):\n",
    "    df[col] = df[col]/value\n",
    "    return df\n",
    "\n",
    "grade_list = ['pk', 'k', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "grade_to_idx = {g: i for i, g in enumerate(grade_list)}\n",
    "\n",
    "def get_range(min_g, max_g):\n",
    "    start_idx = grade_to_idx[min_g]\n",
    "    end_idx = grade_to_idx[max_g]\n",
    "    return ','.join(grade_list[start_idx:end_idx + 1])\n",
    "\n",
    "def get_category_mapping(item_df, input_col, out_put_col,json_file_name):\n",
    "\n",
    "    # Step 1: Preprocess themes (split on commas)\n",
    "    item_df['themes'] = item_df[input_col].fillna('').apply(\n",
    "        lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    "    )\n",
    "    # Step 2: Build theme vocabulary\n",
    "    from itertools import chain\n",
    "    all_themes = sorted(set(chain.from_iterable(item_df['themes'])))\n",
    "    theme_to_idx = {theme: idx for idx, theme in enumerate(all_themes)}\n",
    "    if 'unk' not in theme_to_idx:\n",
    "        theme_to_idx['unk'] = len(theme_to_idx)\n",
    "\n",
    "    # Step 3: Map themes to indices\n",
    "    item_df[out_put_col] = item_df['themes'].apply(\n",
    "        lambda theme_list: [theme_to_idx[t] for t in theme_list if t in theme_to_idx]\n",
    "    )\n",
    "    save_dict_to_json(theme_to_idx,json_file_name)\n",
    "    return item_df, len(theme_to_idx)\n",
    "\n",
    "def get_category_mapping_book(item_df,input_col ,out_put_col,json_file_name):\n",
    "\n",
    "    book_code_to_idx = {theme: idx for idx, theme in enumerate(list((item_df[input_col])))}\n",
    "    if 'unk' not in book_code_to_idx:\n",
    "        book_code_to_idx['unk'] = len(book_code_to_idx)\n",
    "\n",
    "    # Step 3: Map themes to indices\n",
    "    item_df[out_put_col] = item_df[input_col].apply(\n",
    "        lambda theme_list: [book_code_to_idx[t] for t in [theme_list] if t in book_code_to_idx]\n",
    "    )\n",
    "    save_dict_to_json(book_code_to_idx,json_file_name)\n",
    "    return item_df, len(book_code_to_idx)\n",
    "\n",
    "\n",
    "# book_df_final['readable_page_count'] = np.clip(book_df_final['readable_page_count'],0,50)/50\n",
    "# book_series_df.shape\n",
    "\n",
    "def last_10_books_fast(df):\n",
    "    df = df.copy()\n",
    "    df['book_create_dt'] = pd.to_datetime(df['book_create_dt'])\n",
    "    df = df.sort_values(['user_id', 'book_create_dt']).reset_index(drop=True)\n",
    "\n",
    "    # Helper to join last 10 values for each row in a group\n",
    "    def last_10_join(series):\n",
    "        out = []\n",
    "        hist = []\n",
    "        for val in series:\n",
    "            out.append(','.join(hist[-10:]) if hist else 'unk')\n",
    "            hist.append(val)\n",
    "        return pd.Series(out, index=series.index)\n",
    "\n",
    "    # Precompute category/theme strings\n",
    "    # df['cat_str'] = df['category_name'].apply(lambda x: ','.join(x))\n",
    "    # print(df['cat_str'])\n",
    "    # df['theme_str'] = df['theme_name'].apply(lambda x: ','.join(x))\n",
    "    df['cat_str'] = df['category_name']\n",
    "    # print(df['cat_str'])\n",
    "    df['theme_str'] = df['theme_name']\n",
    "    df['rs_str'] = df['reading_skill_name']\n",
    "\n",
    "\n",
    "    # Vectorized per-group computation (one Python loop per group, not per row globally)\n",
    "    grouped = df.groupby('user_id', group_keys=False)\n",
    "    df['last_10_books'] = grouped['book_code'].transform(last_10_join)\n",
    "    df['last_category_name'] = grouped['cat_str'].transform(last_10_join)\n",
    "    df['last_theme_name'] = grouped['theme_str'].transform(last_10_join)\n",
    "    df['last_reading_skill_name'] = grouped['rs_str'].transform(last_10_join)\n",
    "\n",
    "    return df.drop(columns=['cat_str', 'theme_str','rs_str'])\n",
    "\n",
    "def get_mapping_user(child_df , input_col, output_col , file_path ):\n",
    "    category_to_idx = load_json_file(file_path)\n",
    "    child_df['last_categories'] = child_df[input_col].fillna('').apply(\n",
    "    lambda x: [t.strip().lower() for t in x.split(',') if t.strip()])\n",
    "    child_df[output_col] = child_df['last_categories'].apply(\n",
    "    lambda theme_list: [category_to_idx[t] for t in theme_list if t in category_to_idx] )\n",
    "\n",
    "    return child_df, len(category_to_idx)\n",
    "\n",
    "def get_mapping_book_user(child_df , input_col, output_col , file_path ):\n",
    "    book_code_to_idx = load_json_file(file_path)\n",
    "    child_df['last_books_list'] = child_df[input_col].fillna('').apply(\n",
    "    lambda x: [t.strip().lower() for t in x.split(',') if t.strip()]\n",
    ")\n",
    "\n",
    "    child_df[output_col] = child_df['last_books_list'].apply(\n",
    "    lambda theme_list: [book_code_to_idx[int(t)] if t!='unk' else book_code_to_idx[t]  for t in theme_list ])\n",
    "\n",
    "    return child_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def book_data_transformation(book_df):\n",
    "\n",
    "    book_series_df, emb_shape,emb_desc_shape,emb_book_series_shape = pre_train_emb_creation(book_df)\n",
    "\n",
    "    book_series_df = scaling(clip(book_series_df,'readable_page_count',0,50),'readable_page_count',50)\n",
    "\n",
    "    book_series_df['book_type_binary'] = np.where(book_series_df.book_type == 'PDF',1,0)\n",
    "\n",
    "    book_series_df['fiction_nonfiction'].fillna('unk',inplace =True)\n",
    "    book_df_final_v1 = pd.get_dummies(book_series_df, columns=['fiction_nonfiction'], prefix='fn')\n",
    "\n",
    "    book_df_final_v1 = pd.get_dummies(book_df_final_v1, columns=['language_book'], prefix='lang')\n",
    "\n",
    "    book_df_final_v1[\"grades\"] = book_df_final_v1.apply(\n",
    "        lambda row: get_range(row[\"min_grade\"], row[\"max_grade\"]),axis=1)\n",
    "\n",
    "    book_df_final_v1, theme_count = get_category_mapping(book_df_final_v1, 'theme_name', 'theme_ids','theme_to_idx.json')\n",
    "    book_df_final_v1, category_count = get_category_mapping(book_df_final_v1, 'category_name', 'category_ids','category_to_idx.json')\n",
    "    book_df_final_v1, reading_skills_count = get_category_mapping(book_df_final_v1, 'reading_skill_name', 'reading_skill_ids','reading_skill_to_idx.json')\n",
    "    book_df_final_v1, grades_count = get_category_mapping(book_df_final_v1, 'grades', 'grades_ids','grades_to_idx.json')\n",
    "    book_df_final_v1, book_count = get_category_mapping_book(book_df_final_v1,'book_isbn' ,'book_code_ids','book_code_to_idx.json')\n",
    "    book_df_final_v1.rename(columns={'book_isbn':'book_code'},inplace=True)\n",
    "    book_feature_count =  {\n",
    "                    'themes_count':theme_count, \n",
    "                    'book_count': book_count, \n",
    "                    'grade_count': grades_count,\n",
    "                    'reading_skills_count':reading_skills_count,\n",
    "                    'category_count':category_count\n",
    "                   }\n",
    "    emb_count = {\n",
    "                    'themes_count':8, \n",
    "                    'book_count': 16, \n",
    "                    'grade_count': 4,\n",
    "                    'reading_skills_count':4,\n",
    "                    'category_count':4\n",
    "                }\n",
    "    \n",
    "    columns_author_title =[f\"emb_title_author_{i}\" for i in range(emb_shape)]\n",
    "    columns_long_description = [f\"emb_desc_{i}\" for i in range(emb_desc_shape)]\n",
    "    columns_book_series = [f\"emb_book_series_{i}\" for i in range(emb_book_series_shape)]\n",
    "    columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "        'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "        'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "    columns_learn_emb = ['book_code','book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "    book_feature_cols = columns_learn_emb + columns_author_title + columns_long_description + columns_book_series + columns_add  \n",
    "\n",
    "    return book_df_final_v1[book_feature_cols], book_feature_count, emb_count \n",
    "\n",
    "\n",
    "def user_data_transformation(user_df,user_loc,user_platform): \n",
    "    # user_platform.rename(columns ={'isbn':'book_code'},inplace=True)\n",
    "    df1 = user_platform.copy()\n",
    "    user_platform = df1[['user_id', 'book_code', 'book_create_dt',\n",
    "       'cumulative_web_during_school_hour',\n",
    "       'cumulative_web_after_school_hour',\n",
    "       'cumulative_apple_during_school_hour',\n",
    "       'cumulative_apple_after_school_hour',\n",
    "       'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "       'cumulative_unk_during_school_hour',\n",
    "       'cumulative_unk_after_school_hour']]\n",
    "    \n",
    "    user_platform['total'] = user_platform['cumulative_web_during_school_hour'] + user_platform['cumulative_web_after_school_hour'] + user_platform['cumulative_apple_during_school_hour']+ user_platform['cumulative_apple_after_school_hour']+ user_platform['cumulative_android_during_school_hour']+ user_platform['cumulative_android_after_school_hour']+ user_platform['cumulative_unk_during_school_hour']+ user_platform['cumulative_unk_after_school_hour']\n",
    "\n",
    "    user_platform['cumulative_web_during_school_hour'] = user_platform['cumulative_web_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_web_after_school_hour']  = user_platform['cumulative_web_after_school_hour'] /user_platform['total']\n",
    "    user_platform['cumulative_apple_during_school_hour'] = user_platform['cumulative_apple_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_apple_after_school_hour'] = user_platform['cumulative_apple_after_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_android_during_school_hour'] = user_platform['cumulative_android_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_android_after_school_hour'] = user_platform['cumulative_android_after_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_unk_during_school_hour'] = user_platform['cumulative_unk_during_school_hour']/user_platform['total']\n",
    "    user_platform['cumulative_unk_after_school_hour']  = user_platform['cumulative_unk_after_school_hour']/user_platform['total']\n",
    "    \n",
    "    user_platform_final = user_platform[['user_id', 'book_code',\n",
    "       'cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "       'cumulative_apple_during_school_hour',\n",
    "       'cumulative_apple_after_school_hour',\n",
    "       'cumulative_android_during_school_hour', 'cumulative_android_after_school_hour',\n",
    "       'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "       ]]\n",
    "\n",
    "    user_df.dropna(subset=['category_name'],inplace=True)\n",
    "\n",
    "\n",
    "    user_df['category_name'] = user_df['category_name'].fillna('unk')\n",
    "    user_df['theme_name'] = user_df['theme_name'].fillna('unk')\n",
    "    user_df['reading_skill_name'] = user_df['reading_skill_name'].fillna('unk')\n",
    "\n",
    "    user_df['total_pages']=user_df['total_pages'].fillna(user_df['total_pages'].median())\n",
    "    user_df['max_read_pages']=user_df['max_read_pages'].fillna(user_df['max_read_pages'].median())\n",
    "\n",
    "\n",
    "    user_df_v1 = user_df[['book_code', 'user_id','category_name','theme_name','reading_skill_name', 'book_create_dt', 'total_pages',\n",
    "       'max_read_pages']].copy()\n",
    "\n",
    "    user_df_v1['book_create_dt'] = pd.to_datetime(user_df_v1['book_create_dt'])\n",
    "\n",
    "    user_loc_v1 = user_loc[['user_id','country', 'state', 'zipcode','klass_grade_name','teacher_id','school_id','class_activation_bucket']].copy()\n",
    "    \n",
    "    user_raw_df =  user_df_v1.merge(user_loc_v1, how ='left' ,on = 'user_id')\n",
    "\n",
    "   #  user_platform_final['book_code'] = user_platform_final['book_code'].astype('str')\n",
    "\n",
    "    user_raw_df_v1 = user_raw_df.merge(user_platform_final, how ='left' ,on = ['user_id','book_code'])\n",
    "\n",
    "    cv = user_raw_df_v1 .copy()\n",
    "\n",
    "    user_raw_df_v2 = last_10_books_fast(cv)\n",
    "    user_raw_df_v2['class_activation_bucket'] = user_raw_df_v2['class_activation_bucket'].fillna('unk')\n",
    "    user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['klass_grade_name'], prefix='grade')\n",
    "    user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=['class_activation_bucket'], prefix='class_activation_bucket')\n",
    "\n",
    "    user_raw_df_v2['completion_rate'] = user_raw_df_v2['max_read_pages']/user_raw_df_v2['total_pages']\n",
    "    user_raw_df_v2['label'] = np.where(user_raw_df_v2['completion_rate']>0.5,1,0)\n",
    "\n",
    "    user_raw_df_v2, category_count  = get_mapping_user(user_raw_df_v2 , 'last_category_name', 'category_ids' , 'feature_mappings/category_to_idx.json')\n",
    "    user_raw_df_v2, book_count = get_mapping_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "    # user_raw_df_v2, book_count = get_mapping_book_user(user_raw_df_v2 , 'last_10_books', 'book_code_ids' , 'feature_mappings/book_code_to_idx.json')\n",
    "    user_raw_df_v2, reading_skills_count  = get_mapping_user(user_raw_df_v2 , 'last_reading_skill_name', 'reading_skill_ids' , 'feature_mappings/reading_skill_to_idx.json')\n",
    "    user_raw_df_v2, theme_count = get_mapping_user(user_raw_df_v2 , 'last_theme_name', 'theme_ids' , 'feature_mappings/theme_to_idx.json')\n",
    "    user_raw_df_v2, country_count = get_category_mapping(user_raw_df_v2, 'country', 'countries_ids','country_to_idx.json')\n",
    "    user_raw_df_v2, state_count = get_category_mapping(user_raw_df_v2, 'state', 'states_ids','state_to_idx.json')\n",
    "    user_raw_df_v2, zipcode_count = get_category_mapping(user_raw_df_v2, 'zipcode', 'zipcode_ids','zipcode_to_idx.json')\n",
    "    user_raw_df_v2, teacher_count = get_category_mapping(user_raw_df_v2, 'teacher_id', 'teacher_code_ids','teacher_to_idx.json')\n",
    "    user_raw_df_v2, school_count = get_category_mapping(user_raw_df_v2, 'school_id', 'school_code_ids','school_to_idx.json')\n",
    "\n",
    "    user_columns = ['book_code', 'user_id', 'book_create_dt','book_code_ids','category_ids','reading_skill_ids','theme_ids','countries_ids', 'states_ids','zipcode_ids','teacher_code_ids','school_code_ids','state', 'zipcode',\n",
    "       'teacher_id', 'school_id','cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
    "       'cumulative_apple_during_school_hour',\n",
    "       'cumulative_apple_after_school_hour',\n",
    "       'cumulative_android_during_school_hour',\n",
    "       'cumulative_android_after_school_hour',\n",
    "       'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
    "        'grade_grade 1', 'grade_grade 2', 'grade_grade 3',\n",
    "       'grade_grade 4', 'grade_grade 5', 'grade_kindergarten', \n",
    "       'class_activation_bucket_AC', 'class_activation_bucket_AC0',\n",
    "       'class_activation_bucket_AC1', 'class_activation_bucket_AC2',\n",
    "       'class_activation_bucket_AC3', 'class_activation_bucket_unk', 'last_10_books', 'last_category_name', 'last_theme_name',\n",
    "       'last_reading_skill_name','label']\n",
    "\n",
    "    user_feature_count =  {'themes_count':theme_count, \n",
    "                   'book_count': book_count, \n",
    "                   'reading_skills_count':reading_skills_count,\n",
    "                   'category_count':category_count,\n",
    "                   'country_count': country_count , \n",
    "                    'state_count': state_count ,\n",
    "                    'zipcode_count': zipcode_count,\n",
    "                    'teacher_count': teacher_count,\n",
    "                    'school_count': school_count\n",
    "                   }\n",
    "    user_emb_count = {\n",
    "                'themes_count':8, \n",
    "                'book_count': 16, \n",
    "                'reading_skills_count':4,\n",
    "                'category_count':4,\n",
    "                'country_count': 8 , \n",
    "                'state_count': 10,\n",
    "                'zipcode_count': 14,\n",
    "                'teacher_count': 16,\n",
    "                'school_count': 16\n",
    "                }\n",
    "\n",
    "    return user_raw_df_v2[user_columns], user_feature_count, user_emb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c99ba4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'last_10_books_fast' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m child_df,user_feature_count, user_emb_count= \u001b[43muser_data_transformation\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43muser_loc\u001b[49m\u001b[43m,\u001b[49m\u001b[43muser_platform\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36muser_data_transformation\u001b[39m\u001b[34m(user_df, user_loc, user_platform)\u001b[39m\n\u001b[32m     54\u001b[39m user_raw_df_v1 = user_raw_df.merge(user_platform_final, how =\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m ,on = [\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mbook_code\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     56\u001b[39m cv = user_raw_df_v1 .copy()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m user_raw_df_v2 = \u001b[43mlast_10_books_fast\u001b[49m(cv)\n\u001b[32m     59\u001b[39m user_raw_df_v2[\u001b[33m'\u001b[39m\u001b[33mclass_activation_bucket\u001b[39m\u001b[33m'\u001b[39m] = user_raw_df_v2[\u001b[33m'\u001b[39m\u001b[33mclass_activation_bucket\u001b[39m\u001b[33m'\u001b[39m].fillna(\u001b[33m'\u001b[39m\u001b[33munk\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     60\u001b[39m user_raw_df_v2 = pd.get_dummies(user_raw_df_v2, columns=[\u001b[33m'\u001b[39m\u001b[33mklass_grade_name\u001b[39m\u001b[33m'\u001b[39m], prefix=\u001b[33m'\u001b[39m\u001b[33mgrade\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'last_10_books_fast' is not defined"
     ]
    }
   ],
   "source": [
    "child_df,user_feature_count, user_emb_count= user_data_transformation(user_df,user_loc,user_platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a60ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_code</th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_create_dt</th>\n",
       "      <th>book_code_ids</th>\n",
       "      <th>category_ids</th>\n",
       "      <th>state</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>teacher_id</th>\n",
       "      <th>school_id</th>\n",
       "      <th>cumulative_web_during_school_hour</th>\n",
       "      <th>...</th>\n",
       "      <th>class_activation_bucket_AC0</th>\n",
       "      <th>class_activation_bucket_AC1</th>\n",
       "      <th>class_activation_bucket_AC2</th>\n",
       "      <th>class_activation_bucket_AC3</th>\n",
       "      <th>class_activation_bucket_unk</th>\n",
       "      <th>last_10_books</th>\n",
       "      <th>last_category_name</th>\n",
       "      <th>last_theme_name</th>\n",
       "      <th>last_reading_skill_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9781634401647</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-23 20:25:42.214126</td>\n",
       "      <td>[12189]</td>\n",
       "      <td>[8]</td>\n",
       "      <td>CA</td>\n",
       "      <td>93725</td>\n",
       "      <td>DAF7C9F038C84DE3ACD47E581663CDFA</td>\n",
       "      <td>2707F78AAF23426A819369FFA8E0A5D8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "      <td>unk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9781039673106</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:01:06.548406</td>\n",
       "      <td>[2081]</td>\n",
       "      <td>[7]</td>\n",
       "      <td>CA</td>\n",
       "      <td>93725</td>\n",
       "      <td>DAF7C9F038C84DE3ACD47E581663CDFA</td>\n",
       "      <td>2707F78AAF23426A819369FFA8E0A5D8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647</td>\n",
       "      <td>Science &amp; Nature</td>\n",
       "      <td>The Natural World, Fun Science</td>\n",
       "      <td>unk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9781039837843</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:02:48.414486</td>\n",
       "      <td>[2081, 9613]</td>\n",
       "      <td>[7, 1]</td>\n",
       "      <td>CA</td>\n",
       "      <td>93725</td>\n",
       "      <td>DAF7C9F038C84DE3ACD47E581663CDFA</td>\n",
       "      <td>2707F78AAF23426A819369FFA8E0A5D8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647,9781039673106</td>\n",
       "      <td>Science &amp; Nature,Early Learning</td>\n",
       "      <td>The Natural World, Fun Science,Alphabet, Langu...</td>\n",
       "      <td>unk,Illustrations or other Visual Elements, De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9781634409636</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:04:49.676354</td>\n",
       "      <td>[2081, 9613, 6130]</td>\n",
       "      <td>[7, 1, 5]</td>\n",
       "      <td>CA</td>\n",
       "      <td>93725</td>\n",
       "      <td>DAF7C9F038C84DE3ACD47E581663CDFA</td>\n",
       "      <td>2707F78AAF23426A819369FFA8E0A5D8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647,9781039673106,9781039837843</td>\n",
       "      <td>Science &amp; Nature,Early Learning,Growing Up</td>\n",
       "      <td>The Natural World, Fun Science,Alphabet, Langu...</td>\n",
       "      <td>unk,Illustrations or other Visual Elements, De...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9781634409759</td>\n",
       "      <td>00000ac6-1a89-415c-8d67-177e17aa1aae</td>\n",
       "      <td>2024-09-24 20:05:17.458130</td>\n",
       "      <td>[2081, 9613, 6130, 2221]</td>\n",
       "      <td>[7, 1, 5, 6]</td>\n",
       "      <td>CA</td>\n",
       "      <td>93725</td>\n",
       "      <td>DAF7C9F038C84DE3ACD47E581663CDFA</td>\n",
       "      <td>2707F78AAF23426A819369FFA8E0A5D8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>9781634401647,9781039673106,9781039837843,9781...</td>\n",
       "      <td>Science &amp; Nature,Early Learning,Growing Up,Peo...</td>\n",
       "      <td>The Natural World, Fun Science,Alphabet, Langu...</td>\n",
       "      <td>unk,Illustrations or other Visual Elements, De...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       book_code                               user_id  \\\n",
       "0  9781634401647  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "1  9781039673106  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "2  9781039837843  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "3  9781634409636  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "4  9781634409759  00000ac6-1a89-415c-8d67-177e17aa1aae   \n",
       "\n",
       "              book_create_dt             book_code_ids  category_ids state  \\\n",
       "0 2024-09-23 20:25:42.214126                   [12189]           [8]    CA   \n",
       "1 2024-09-24 20:01:06.548406                    [2081]           [7]    CA   \n",
       "2 2024-09-24 20:02:48.414486              [2081, 9613]        [7, 1]    CA   \n",
       "3 2024-09-24 20:04:49.676354        [2081, 9613, 6130]     [7, 1, 5]    CA   \n",
       "4 2024-09-24 20:05:17.458130  [2081, 9613, 6130, 2221]  [7, 1, 5, 6]    CA   \n",
       "\n",
       "  zipcode                        teacher_id                         school_id  \\\n",
       "0   93725  DAF7C9F038C84DE3ACD47E581663CDFA  2707F78AAF23426A819369FFA8E0A5D8   \n",
       "1   93725  DAF7C9F038C84DE3ACD47E581663CDFA  2707F78AAF23426A819369FFA8E0A5D8   \n",
       "2   93725  DAF7C9F038C84DE3ACD47E581663CDFA  2707F78AAF23426A819369FFA8E0A5D8   \n",
       "3   93725  DAF7C9F038C84DE3ACD47E581663CDFA  2707F78AAF23426A819369FFA8E0A5D8   \n",
       "4   93725  DAF7C9F038C84DE3ACD47E581663CDFA  2707F78AAF23426A819369FFA8E0A5D8   \n",
       "\n",
       "   cumulative_web_during_school_hour  ...  class_activation_bucket_AC0  \\\n",
       "0                                1.0  ...                        False   \n",
       "1                                1.0  ...                        False   \n",
       "2                                1.0  ...                        False   \n",
       "3                                1.0  ...                        False   \n",
       "4                                1.0  ...                        False   \n",
       "\n",
       "   class_activation_bucket_AC1  class_activation_bucket_AC2  \\\n",
       "0                        False                        False   \n",
       "1                        False                        False   \n",
       "2                        False                        False   \n",
       "3                        False                        False   \n",
       "4                        False                        False   \n",
       "\n",
       "   class_activation_bucket_AC3  class_activation_bucket_unk  \\\n",
       "0                         True                        False   \n",
       "1                         True                        False   \n",
       "2                         True                        False   \n",
       "3                         True                        False   \n",
       "4                         True                        False   \n",
       "\n",
       "                                       last_10_books  \\\n",
       "0                                                unk   \n",
       "1                                      9781634401647   \n",
       "2                        9781634401647,9781039673106   \n",
       "3          9781634401647,9781039673106,9781039837843   \n",
       "4  9781634401647,9781039673106,9781039837843,9781...   \n",
       "\n",
       "                                  last_category_name  \\\n",
       "0                                                unk   \n",
       "1                                   Science & Nature   \n",
       "2                    Science & Nature,Early Learning   \n",
       "3         Science & Nature,Early Learning,Growing Up   \n",
       "4  Science & Nature,Early Learning,Growing Up,Peo...   \n",
       "\n",
       "                                     last_theme_name  \\\n",
       "0                                                unk   \n",
       "1                     The Natural World, Fun Science   \n",
       "2  The Natural World, Fun Science,Alphabet, Langu...   \n",
       "3  The Natural World, Fun Science,Alphabet, Langu...   \n",
       "4  The Natural World, Fun Science,Alphabet, Langu...   \n",
       "\n",
       "                             last_reading_skill_name  label  \n",
       "0                                                unk      0  \n",
       "1                                                unk      1  \n",
       "2  unk,Illustrations or other Visual Elements, De...      1  \n",
       "3  unk,Illustrations or other Visual Elements, De...      0  \n",
       "4  unk,Illustrations or other Visual Elements, De...      1  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709beea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_author_title =[f\"emb_title_author_{i}\" for i in range(384)]\n",
    "columns_long_description = [f\"emb_desc_{i}\" for i in range(384)]\n",
    "columns_book_series = [f\"emb_book_series_{i}\" for i in range(384)]\n",
    "columns_add = ['readable_page_count','book_type_binary', 'fn_Fiction', 'fn_Non-Fiction', 'fn_unk',\n",
    "       'lang_English', 'lang_French', 'lang_Haitian French Creole',\n",
    "       'lang_Mandarin', 'lang_Portuguese', 'lang_Spanish']\n",
    "\n",
    "columns_learn_emb = ['book_code_ids','grades_ids','reading_skill_ids', 'category_ids','theme_ids']\n",
    "\n",
    "book_feature_cols = columns_author_title + columns_long_description + columns_book_series + columns_add\n",
    "\n",
    "interaction_feature_cols = ['cumulative_web_during_school_hour',\n",
    " 'cumulative_web_after_school_hour',\n",
    " 'cumulative_apple_during_school_hour',\n",
    " 'cumulative_apple_after_school_hour',\n",
    " 'cumulative_android_during_school_hour',\n",
    " 'cumulative_android_after_school_hour',\n",
    " 'cumulative_unk_during_school_hour',\n",
    " 'cumulative_unk_after_school_hour',\n",
    " 'grade_grade 1',\n",
    " 'grade_grade 2',\n",
    " 'grade_grade 3',\n",
    " 'grade_grade 4',\n",
    " 'grade_grade 5',\n",
    " 'grade_kindergarten',\n",
    " 'class_activation_bucket_AC',\n",
    " 'class_activation_bucket_AC0',\n",
    " 'class_activation_bucket_AC1',\n",
    " 'class_activation_bucket_AC2',\n",
    " 'class_activation_bucket_AC3',\n",
    " 'class_activation_bucket_unk',]\n",
    "\n",
    "\n",
    "\n",
    "len(book_feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.main_utils import  fast_split_user_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test= fast_split_user_interactions(child_df[:100000], user_col=\"user_id\", item_col=\"book_code\", time_col=\"book_create_dt\", min_interactions=10, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed669186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index= False)\n",
    "val.to_csv('val.csv',index= False)\n",
    "test.to_csv('test.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c724541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9b92484",
   "metadata": {},
   "outputs": [],
   "source": [
    "train =  pd.read_csv('train.csv')\n",
    "val =  pd.read_csv('val.csv')\n",
    "test =  pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8838639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['book_code', 'user_id', 'book_create_dt', 'book_code_ids',\n",
       "       'category_ids', 'state', 'zipcode', 'teacher_id', 'school_id',\n",
       "       'cumulative_web_during_school_hour', 'cumulative_web_after_school_hour',\n",
       "       'cumulative_apple_during_school_hour',\n",
       "       'cumulative_apple_after_school_hour',\n",
       "       'cumulative_android_during_school_hour',\n",
       "       'cumulative_android_after_school_hour',\n",
       "       'cumulative_unk_during_school_hour', 'cumulative_unk_after_school_hour',\n",
       "       'grade_grade 1', 'grade_grade 2', 'grade_grade 3', 'grade_grade 4',\n",
       "       'grade_grade 5', 'grade_kindergarten', 'class_activation_bucket_AC',\n",
       "       'class_activation_bucket_AC0', 'class_activation_bucket_AC1',\n",
       "       'class_activation_bucket_AC2', 'class_activation_bucket_AC3',\n",
       "       'class_activation_bucket_unk', 'last_10_books', 'last_category_name',\n",
       "       'last_theme_name', 'last_reading_skill_name', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c829c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.data_loader import BookInteractionDataset, book_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610de7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3802477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = BookInteractionDataset(train, item_df, book_feature_cols, interaction_feature_cols)\n",
    "trainloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=book_collate_fn)\n",
    "\n",
    "dataset = BookInteractionDataset( val, item_df, book_feature_cols, interaction_feature_cols)\n",
    "valloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=book_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b9b939",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'theme_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'theme_ids'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recommendation_system/Two-Tower/src/components/data_loader.py:36\u001b[39m, in \u001b[36mBookInteractionDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     33\u001b[39m book_code_ids= book_info[\u001b[33m'\u001b[39m\u001b[33mbook_code_ids\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# already list[int]\u001b[39;00m\n\u001b[32m     35\u001b[39m last_book_ids = row[\u001b[33m'\u001b[39m\u001b[33mbook_code_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m last_theme_ids = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtheme_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     37\u001b[39m last_category_ids = row[\u001b[33m'\u001b[39m\u001b[33mcategory_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     38\u001b[39m last_reading_skills_id = row[\u001b[33m'\u001b[39m\u001b[33mreading_skill_ids\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/series.py:1130\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/series.py:1246\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'theme_ids'"
     ]
    }
   ],
   "source": [
    "for i in trainloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca33a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.model import TwoTowerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc349f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'themes_count': 8,\n",
       " 'book_count': 16,\n",
       " 'grade_count': 4,\n",
       " 'reading_skills_count': 4,\n",
       " 'category_count': 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44c5f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoTowerModel(book_feature_count, user_feature_count,\n",
    "                 emb_count, user_emb_count,\n",
    "                  book_feature_dim=len(book_feature_cols), user_feature_dim=len(interaction_feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f97359cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components.model_trainer import train_two_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bca916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/11303 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'theme_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'theme_ids'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain_two_tower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Your PyTorch DataLoader for training\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Your PyTorch DataLoader for validation\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# Number of epochs\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                        \u001b[49m\u001b[38;5;66;43;03m# Learning rate\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# L2 regularization\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Early stopping patience                  # \"cuda\" or \"cpu\"\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./checkpoints\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Where to save the best model\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtwo_tower_best.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_k_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# Compute Recall@5, Recall@10, NDCG@5, NDCG@10\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recommendation_system/Two-Tower/src/components/model_trainer.py:49\u001b[39m, in \u001b[36mtrain_two_tower\u001b[39m\u001b[34m(model, train_loader, val_loader, epochs, lr, weight_decay, patience, device, checkpoint_dir, checkpoint_name, eval_k_list)\u001b[39m\n\u001b[32m     47\u001b[39m model.train()\n\u001b[32m     48\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/recommendation_system/Two-Tower/src/components/data_loader.py:36\u001b[39m, in \u001b[36mBookInteractionDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     33\u001b[39m book_code_ids= book_info[\u001b[33m'\u001b[39m\u001b[33mbook_code_ids\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# already list[int]\u001b[39;00m\n\u001b[32m     35\u001b[39m last_book_ids = row[\u001b[33m'\u001b[39m\u001b[33mbook_code_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m last_theme_ids = \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtheme_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     37\u001b[39m last_category_ids = row[\u001b[33m'\u001b[39m\u001b[33mcategory_ids\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     38\u001b[39m last_reading_skills_id = row[\u001b[33m'\u001b[39m\u001b[33mreading_skill_ids\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/series.py:1130\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/series.py:1246\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/tta/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'theme_ids'"
     ]
    }
   ],
   "source": [
    "model = train_two_tower(\n",
    "    model=model,\n",
    "    train_loader=trainloader,      # Your PyTorch DataLoader for training\n",
    "    val_loader=valloader,          # Your PyTorch DataLoader for validation\n",
    "    epochs=2,                      # Number of epochs\n",
    "    lr=1e-3,                        # Learning rate\n",
    "    weight_decay=1e-5,              # L2 regularization\n",
    "    patience=5,                     # Early stopping patience                  # \"cuda\" or \"cpu\"\n",
    "    checkpoint_dir=\"./checkpoints\", # Where to save the best model\n",
    "    checkpoint_name=\"two_tower_best.pt\",\n",
    "    eval_k_list=[3, 5]              # Compute Recall@5, Recall@10, NDCG@5, NDCG@10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb2ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
